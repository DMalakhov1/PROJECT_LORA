# **Lora под задачу на русском языке**

# **Предисловие**

Я старался максимально расписывать код, так что его запустить сможет буквально каждый. 
В ноутбуках очень много хэштэгов с пояснениями. Тетрадки собраны из многих моих черновиков, 
поэтому они могут показаться длинными. Возможно код не идеален, но он работает, чему я безусловно рад.

# **Личный мотив**
Для меня было интересно опробовать этот инструмент, поэтому постарался сделать его хотя бы не скучным,
чтобы во время написания кода, мне не хотелось плакать (хотелось) 

# **Информация по файлам**

***output.jsonl*** - размеченный датасет на котором модель доолучалась (размер 8641)

***data_small.jsonl*** - это датасет output.jsonl, но уменьшенный в 20 раз, использовался 
в песочнице, чтобы не тратить все свои ресурсы на Google Colab 

**LORA_END-2.ipynb** - это тетрадь, в которой был показан весь мой путь до работающего кода, используемые 
модели в этой тетрадки небольшие, но зато неплохо подходили для самого начала
- sberbank-ai/rugpt3small_based_on_gpt2
- berbank-ai/rugpt3medium_based_on_gpt2

**QVikhr-3-1.7B.ipynb** - это тетрадь, где обучалась моя основная одноименная модель, и откуда были получены все веса и тд 
(эта тетрадь основана на тетради LORA_END-2.ipynb так что можно сказать, эта тертрадь универсальный конструктор :) 

**Инструкция по запуску LORA**

1. Берем тетрадь и ее тупо запускаем все все все ячейки друг за другом (конфликтов не должно быть)


   Что должно произойти? Модель начнет обучаться

2. Если все ячейки будут запущены --> в самом конце будет блок с вопросами, которые мы хотим задать
3. Радуйтесь, что работает - я лично радовался



