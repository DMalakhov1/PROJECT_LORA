{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Введение**\n",
    "\n",
    "Всем привет, это моя тетрадь по дообучению с LORA, эта тетрадь собрана из многих моих тетрадей. \n",
    "Здесь я постараюсь показать как я дошел до своего решения. Всем приятного чтения! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Штука для гугл коллаба**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Скучная, но необходимая часть - догрузка и  импорт Библиотек**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Догрузка**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zrh5kxTWiA5l",
    "outputId": "45793a54-041c-44fa-d237-24c7fa0e5ead"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers\n",
    "!pip install -q peft\n",
    "!pip install -q bitsandbytes\n",
    "!pip install -q datasets --upgrade \n",
    "!pip install -q accelerate\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q torch\n",
    "!pip install -q numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Библиотека**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import json\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import os # Для создания директории\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "import os # Для проверки пути\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Проверка GPU/CPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2A7a9zmPhCet",
    "outputId": "9f374e1b-c8a8-487c-a6fa-eaa96bc3b9b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU доступен: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU доступен: {gpu_name}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"GPU не доступен\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deBL_MnHixJP",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Загрузка модели и токенизатора (с 4-битным квантованием)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wBP-TOL3iyDe",
    "outputId": "bb123839-c5c7-4501-9bb4-21fecd3fd738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель 'sberbank-ai/rugpt3small_based_on_gpt2' успешно загружена с 4-битным квантованием.\n",
      "\n",
      "Информация о модели:\n",
      "  Количество параметров: 125.23M (после квантования может быть меньше)\n",
      "  Конфигурация модели: GPT2Config\n",
      "\n",
      "Информация о токенизаторе:\n",
      "  Класс токенизатора: GPT2TokenizerFast\n",
      "  Размер словаря: 50257\n",
      "  Pad token: <pad> (ID: 0)\n",
      "  EOS token: </s> (ID: 2)\n",
      "  BOS token: <s> (ID: 1)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
    "# model_name = \"sberbank-ai/rugpt3medium_based_on_gpt2\" - как показала практика буду использовать эту модель. но не суть \n",
    "\n",
    "# Конфигурация для 4-битного квантования\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", # Тип квантования - это база (часто используют nf4)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, \n",
    "    bnb_4bit_use_double_quant=True, # Использовать двойное квантование для еще большей экономии, экономим максимально \n",
    ")\n",
    "\n",
    "# Загрузка модели с квантованием\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16, \n",
    "        device_map=\"auto\", \n",
    "        trust_remote_code=True # \n",
    "    )\n",
    "    print(f\"Модель '{model_name}' успешно загружена с 4-битным квантованием ураааа\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при загрузке модели: {e}\")\n",
    "    print(\"Попробуем загрузить без явного torch_dtype=torch.bfloat16, если ошибка связана с ним\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################___Еще_одна_вариация_###########################################\n",
    "    \n",
    "    # Иногда на T4 bfloat16 может вызывать проблемы, попробуем с float16, поэтому попробуем след финт ушами \n",
    "    bnb_config_fp16 = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16, # изменение только тут \n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config_fp16,\n",
    "        torch_dtype=torch.float16, \n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(f\"Модель '{model_name}' успешно загружена с 4-битным квантованием и compute_dtype=torch.float16.\")\n",
    "\n",
    "\n",
    "\n",
    "######################################__________ТОКИНИЗАТОР___________###################################\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Настройка токенизатора GPT-2 модели часто не имеют специального pad_token\n",
    "# Мы можем использовать eos_token (end-of-sequence) в качестве pad_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"Установлен pad_token: {tokenizer.pad_token} (был None, использован eos_token)\")\n",
    "\n",
    "# И уже по цепочке надо обновим конфигурацию модели, если pad_token_id не был установлен\n",
    "if model.config.pad_token_id is None:\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    print(f\"Установлен model.config.pad_token_id: {model.config.pad_token_id}\")\n",
    "\n",
    "\n",
    "############__Я_стараюсь_написать_ну_прям_очень_наглядный код,_поэтому_буду_выводить_все_в_этом_мире_########\n",
    "\n",
    "print(\"\\nИнформация о модели:\")\n",
    "print(f\"  Количество параметров: {model.num_parameters() / 1e6:.2f}M (после квантования может быть меньше)\") # num_parameters может не отражать фактический размер в памяти после квантования\n",
    "print(f\"  Конфигурация модели: {model.config.__class__.__name__}\")\n",
    "\n",
    "print(\"\\nИнформация о токенизаторе:\")\n",
    "print(f\"  Класс токенизатора: {tokenizer.__class__.__name__}\")\n",
    "print(f\"  Размер словаря: {tokenizer.vocab_size}\")\n",
    "print(f\"  Pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"  BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\") # Обычно нет у GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sW_lfLvbjQ0J",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Что у нас по данным, Джонии? Загрузка и просмотр данных ₍^. .^₎⟆**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J0R2Bp5PjRzZ",
    "outputId": "ca7da48e-58a3-420b-9cba-73b558cb3150"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Успешно загружено 8641 записей из /content/output.jsonl\n",
      "\n",
      "Первые 3 примера из датасета:\n",
      "{'instruction': 'Какую тему в интервью с Балакиной Татьяной Петровной обсудили в первую очередь?', 'input': '', 'output': 'Карьерный путь, науку и образование.'}\n",
      "{'instruction': 'Какие направления в образовании упомянуты в контексте обсуждения с Татьяной Петровной?', 'input': '', 'output': 'Экономика и физика.'}\n",
      "{'instruction': 'Почему важно участвовать в олимпиадах и хакатонах по мнению Татьяны Петровны?', 'input': '', 'output': 'Ответ на этот вопрос можно найти в интервью, но в данном тексте он не представлен.'}\n",
      "\n",
      "Информация о датасете:\n",
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output'],\n",
      "    num_rows: 8641\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/content/output.jsonl\" # Путь к вашему файлу, пожалуйста, настройка под себя, укажите свой путь\n",
    "data = []\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    print(f\"Успешно загружено {len(data)} записей из {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при чтении файла {file_path}: {e}\")\n",
    "\n",
    "if data:\n",
    "    dataset = Dataset.from_list(data)\n",
    "\n",
    "    print(\"\\nПервые 3 примера из датасета:\")\n",
    "    for i in range(min(3, len(dataset))):\n",
    "        print(dataset[i])\n",
    "\n",
    "    # Инфа о датасете\n",
    "    print(\"\\nИнформация о датасете:\")\n",
    "    print(dataset)\n",
    "else:\n",
    "    print(\"Данные не были загружены. Проверьте путь к файлу и его содержимое и прочитайте самый первый мой комментарий\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJb65-aqktob",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Форматирование данных и создание текстовых промптов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513,
     "referenced_widgets": [
      "0fcc8377aa6d45deb4d6f5fc6b543a39",
      "bb94101f510a4305aeacb3a6d3116614",
      "f1ff745c43a54156a3f3f5816298a208",
      "f97846cf95304ebab1de87e3234486bd",
      "63063222c5854c888aea65436bef382a",
      "4712036c6a4145e1b555f7ac07f8aca0",
      "ff4f9192b05b4ba8ae46eac8e7b694c2",
      "e30fd5a1258b4437b2ae18fdac3bbff9",
      "22c294b3d2484e0b96ce730dcd5057b1",
      "d72e5de082104630a60176f188267330",
      "a2c6f74126374f6fa39213ca51e413b6"
     ]
    },
    "id": "FOovINTgkPzQ",
    "outputId": "23ced1e9-bc4e-42af-854d-6de2a912e7c0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fcc8377aa6d45deb4d6f5fc6b543a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Первые 2 примера сгенерированных текстовых промптов (колонка 'text'):\n",
      "--- Пример 1 ---\n",
      "Инструкция: Какую тему в интервью с Балакиной Татьяной Петровной обсудили в первую очередь?\n",
      "\n",
      "Ответ: Карьерный путь, науку и образование.</s>\n",
      "--------------------\n",
      "--- Пример 2 ---\n",
      "Инструкция: Какие направления в образовании упомянуты в контексте обсуждения с Татьяной Петровной?\n",
      "\n",
      "Ответ: Экономика и физика.</s>\n",
      "--------------------\n",
      "\n",
      "Информация о датасете с текстовыми промптами:\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 8641\n",
      "})\n",
      "\n",
      "Пример первой записи (ключи):\n",
      "dict_keys(['text'])\n",
      "\n",
      "Текст первой записи:\n",
      "Инструкция: Какую тему в интервью с Балакиной Татьяной Петровной обсудили в первую очередь?\n",
      "\n",
      "Ответ: Карьерный путь, науку и образование.</s>\n"
     ]
    }
   ],
   "source": [
    "# Проверка существования и заполненности датасета (да, в какой-то момент у меня были с этим проблемы)\n",
    "if 'dataset' not in globals() or not dataset:\n",
    "    print(\"Warning, исходный датасет не был загружен или пуст\")\n",
    "else:\n",
    "    eos_token = tokenizer.eos_token  # Получаем eos_token из токенизатора\n",
    "    if eos_token is None:\n",
    "        print(\"Предупреждение: eos_token не найден, подумай почему\")\n",
    "        eos_token = \"\"\n",
    "\n",
    "    def create_prompt_text(example):\n",
    "        prompt_template = f\"Инструкция: {example['instruction']}\\n\\nОтвет: {example['output']}{eos_token}\"\n",
    "        return {\"text\": prompt_template}\n",
    "\n",
    "    # Применяем функцию ко всему датасету для создания новой колонки 'text'\n",
    "    dataset_with_text_prompts = dataset.map(\n",
    "        create_prompt_text,\n",
    "        remove_columns=['instruction', 'input', 'output']\n",
    "    )\n",
    "\n",
    "    print(\"\\nПервые 3 примера сгенерированных текстовых промптов\")\n",
    "    for i in range(min(3, len(dataset_with_text_prompts))):\n",
    "        print(f\"--- Пример {i+1} ---\")\n",
    "        print(dataset_with_text_prompts[i]['text'])\n",
    "        print(\"--------------------\")\n",
    "\n",
    "    print(\"\\nИнформация о датасете с текстовыми промптами:\")\n",
    "    print(dataset_with_text_prompts)\n",
    "    if len(dataset_with_text_prompts) > 0:\n",
    "        print(\"\\nПример первой записи (ключи):\")\n",
    "        print(dataset_with_text_prompts[0].keys())\n",
    "        print(\"\\nТекст первой записи:\")\n",
    "        print(dataset_with_text_prompts[0]['text'])\n",
    "    else:\n",
    "        print(\"Датасет с текстовыми промптами пуст, проверяй где накосячил\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Поработаем с токенизацией, далее поделим на train/val + установим формат**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228,
     "referenced_widgets": [
      "c1770e9c8dcd4f07842192d3560135c0",
      "e3ebbc6db77a41c2a8fffc26ad5668b5",
      "2543f71daf8e4e398b5335d5f7aa0679",
      "62cf65ceb7814cd5a4f34cbd710b2d0c",
      "bb619c8c364f49ab9773a29756bb30ec",
      "243ca11ae232445e942dc676a088bab5",
      "3a6bf958401d49448f334d18d5df740c",
      "697df8de2b2c48bc8bb45501a0de2657",
      "ae2fa80983824a708e0a71a6c1658b30",
      "76642c05ab754cfa9af73e317a239576",
      "501f55cee3eb450f934b2631e10acc6e"
     ]
    },
    "id": "VJnu7XaPkTAr",
    "outputId": "de6f45a4-17f1-4fb5-a980-7c8ee8ec5483"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1770e9c8dcd4f07842192d3560135c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Информация о полном токенизированном датасете (до разделения):\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 8641\n",
      "})\n",
      "\n",
      "Датасет успешно разделен на обучающую и валидационную выборки.\n",
      "Размер обучающей выборки: 7776\n",
      "Размер валидационной выборки: 865\n"
     ]
    }
   ],
   "source": [
    "#########################_Проверка_мол_все_ли_у_нас_хорошо_#########################\n",
    "\n",
    "if 'dataset_with_text_prompts' not in globals() or not dataset_with_text_prompts:\n",
    "    print(\"Ошибка: 'dataset_with_text_prompts' не найден, ищи ошибку\")\n",
    "else:\n",
    "    def tokenize_function(examples):\n",
    "\n",
    "        # Пояснительная бригада, что я натыкал и для чего \n",
    "\n",
    "        # max_length можно будет подобрать позже, пока оставим значение по умолчанию токенизатора\n",
    "        # или установим достаточно большое, чтобы вместить большинство наших промптов, но пока 512\n",
    "        # truncation=True обрежет слишком длинные последовательности\n",
    "        # padding=\"max_length\" добавит паддинг до max_length\n",
    "\n",
    "        \n",
    "        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Токенизируем тексты\n",
    "    tokenized_dataset_full = dataset_with_text_prompts.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "    print(\"\\nИнформация о полном токенизированном датасете до разделения на train/val\")\n",
    "    print(tokenized_dataset_full)\n",
    "\n",
    "#########################_Делим_данные_на_train/val_#########################\n",
    "    try:\n",
    "        split_dataset = tokenized_dataset_full.train_test_split(test_size=0.1, seed=42)\n",
    "        train_dataset = split_dataset['train']\n",
    "        eval_dataset = split_dataset['test']\n",
    "        train_dataset.set_format(\"torch\")\n",
    "        eval_dataset.set_format(\"torch\")\n",
    "\n",
    "        print(\"\\nДатасет успешно разделен train/val\")\n",
    "        print(f\"Размер train: {len(train_dataset)}\")\n",
    "        print(f\"Размер val: {len(eval_dataset)}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Произошла ошибка, иди исправляй: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1Gl-Nw7k7D6"
   },
   "source": [
    "# **Конфигурация LoRA и QLoRA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUwghdABk9XR",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Подготовка модели к PEFT и конфигурация LoRA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w5i8F1Msk3ub",
    "outputId": "c8062deb-6ab0-4dbe-8db3-296817b95548"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель, токенизатор и датасеты готовы.\n",
      "Модель подготовлена для k-bit training (QLoRA).\n",
      "\n",
      "LoraConfig создана:\n",
      "LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=16, target_modules={'c_attn'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n",
      "\n",
      "LoRA применена к модели.\n",
      "\n",
      "Информация о параметрах после применения LoRA:\n",
      "trainable params: 589,824 || all params: 125,821,440 || trainable%: 0.4688\n",
      "\n",
      "Модель готова к обучению с LoRA.\n"
     ]
    }
   ],
   "source": [
    "#########################_Проверка_мол_все_ли_у_нас_хорошо_#########################\n",
    "\n",
    "if 'model' not in globals() or 'tokenizer' not in globals():\n",
    "    print(\"Warning: Модель или токенизатор не загружены, ищи ошибку\")\n",
    "elif 'train_dataset' not in globals() or 'eval_dataset' not in globals():\n",
    "    print(\"Warning: Обучающий или валидационный датасет не готовы\")\n",
    "else:\n",
    "    print(\"Модель, токенизатор и датасеты готовы\")\n",
    "\n",
    "#########################_Подготовка модели для k-битного обучения_#########################\n",
    "\n",
    "    # Это необходимо для стабильности при использовании QLoRA\n",
    "    # Добавляет необходимые хуки и изменяет некоторые слои для совместимости с LoRA поверх квантованных весов\n",
    "    try:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        print(\"Модель подготовлена для k-bit training QLoRA\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при prepare_model_for_kbit_training: {e}\")\n",
    "        print(\"Продолжаем без этого шага, но могут быть проблемы со стабильностью QLoRA, так что думайте\")\n",
    "\n",
    "#########################_Определение_конфигурации_LoRA_#########################\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,  # Ранг LoRA-матриц, больше параметров, потенциально лучше, но дольше обучение.\n",
    "        lora_alpha=32, # Альфа для масштабирования LoRA\n",
    "        target_modules=[\n",
    "            \"c_attn\", # Для GPT-2 это слои query, key, value в self-attention.\n",
    "                      # Для других моделей могут быть другие названия, например, \"q_proj\", \"v_proj\", \"k_proj\"\n",
    "            # \"c_proj\", # Можно добавить и другие линейные слои, если необходимо\n",
    "            # \"c_fc\"    # Слой в feed-forward сети\n",
    "        ],\n",
    "        lora_dropout=0.05, \n",
    "        bias=\"none\", # \"none\" обычно хорошо работает\n",
    "        task_type=\"CAUSAL_LM\" # Тип задачи \n",
    "    )\n",
    "    print(\"\\nLoraConfig создана:\")\n",
    "    print(lora_config)\n",
    "\n",
    "\n",
    "#########################_Применение_LoRA_к_модели_#########################\n",
    "    \n",
    "    try:\n",
    "        lora_model = get_peft_model(model, lora_config)\n",
    "        print(\"\\nLoRA применена, красава\")\n",
    "        print(\"\\nИнформация о параметрах после применения LoRA\")\n",
    "        lora_model.print_trainable_parameters()\n",
    "        model_to_train = lora_model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при применении LoRA к модели: {e}\")\n",
    "        model_to_train = None \n",
    "\n",
    "    # Проверим, что модель готова для обучения\n",
    "    if model_to_train:\n",
    "        print(\"\\nМодель готова к обучению с LoRA\")\n",
    "    else:\n",
    "        print(\"\\nМодель НЕ готова к обучению, чек ошибки выше\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pbBajq_lJNP"
   },
   "source": [
    "Обучение (Fine-tuning).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ygw28TAllLvD",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Настройка аргументов обучения + запуск Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0jwulTPLlKCN",
    "outputId": "847e20d0-ea5c-44df-8798-393ded583ef7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-efc7458c410e>:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Все компоненты для обучения готовы.\n",
      "\n",
      "Trainer сконфигурирован. Начинаем обучение...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='206' max='486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [206/486 04:21 < 05:59, 0.78 it/s, Epoch 0.42/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.493900</td>\n",
       "      <td>2.339689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.467100</td>\n",
       "      <td>2.328710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='486' max='486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [486/486 10:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.493900</td>\n",
       "      <td>2.339689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.467100</td>\n",
       "      <td>2.328710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.450400</td>\n",
       "      <td>2.324002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.434200</td>\n",
       "      <td>2.320222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Обучение завершено!\n",
      "Финальные LoRA адаптеры сохранены в: ./results_rugpt3small_lora_qa/final_model_adapters\n",
      "\n",
      "Метрики обучения:\n",
      "{'train_runtime': 609.6727, 'train_samples_per_second': 12.754, 'train_steps_per_second': 0.797, 'total_flos': 2045896481046528.0, 'train_loss': 2.455147182009348, 'epoch': 1.0}\n",
      "\n",
      "Лог обучения (история):\n",
      "{'loss': 2.4901, 'grad_norm': 1.233849287033081, 'learning_rate': 8.991769547325102e-05, 'epoch': 0.102880658436214, 'step': 50}\n",
      "{'loss': 2.4939, 'grad_norm': 1.4035499095916748, 'learning_rate': 7.962962962962964e-05, 'epoch': 0.205761316872428, 'step': 100}\n",
      "{'eval_loss': 2.339689016342163, 'eval_runtime': 18.2246, 'eval_samples_per_second': 47.463, 'eval_steps_per_second': 5.981, 'epoch': 0.205761316872428, 'step': 100}\n",
      "{'loss': 2.4752, 'grad_norm': 1.2862788438796997, 'learning_rate': 6.934156378600823e-05, 'epoch': 0.30864197530864196, 'step': 150}\n",
      "{'loss': 2.4671, 'grad_norm': 1.3951125144958496, 'learning_rate': 5.905349794238684e-05, 'epoch': 0.411522633744856, 'step': 200}\n",
      "{'eval_loss': 2.328709840774536, 'eval_runtime': 18.1933, 'eval_samples_per_second': 47.545, 'eval_steps_per_second': 5.991, 'epoch': 0.411522633744856, 'step': 200}\n",
      "{'loss': 2.453, 'grad_norm': 1.334975004196167, 'learning_rate': 4.876543209876544e-05, 'epoch': 0.51440329218107, 'step': 250}\n",
      "{'loss': 2.4504, 'grad_norm': 1.2632235288619995, 'learning_rate': 3.8477366255144036e-05, 'epoch': 0.6172839506172839, 'step': 300}\n",
      "{'eval_loss': 2.3240020275115967, 'eval_runtime': 18.1783, 'eval_samples_per_second': 47.584, 'eval_steps_per_second': 5.996, 'epoch': 0.6172839506172839, 'step': 300}\n",
      "{'loss': 2.444, 'grad_norm': 1.4628962278366089, 'learning_rate': 2.8189300411522634e-05, 'epoch': 0.720164609053498, 'step': 350}\n",
      "{'loss': 2.4342, 'grad_norm': 1.2720030546188354, 'learning_rate': 1.7901234567901236e-05, 'epoch': 0.823045267489712, 'step': 400}\n",
      "{'eval_loss': 2.3202219009399414, 'eval_runtime': 18.2049, 'eval_samples_per_second': 47.515, 'eval_steps_per_second': 5.987, 'epoch': 0.823045267489712, 'step': 400}\n",
      "{'loss': 2.4055, 'grad_norm': 1.2075163125991821, 'learning_rate': 7.613168724279836e-06, 'epoch': 0.9259259259259259, 'step': 450}\n",
      "{'train_runtime': 609.6727, 'train_samples_per_second': 12.754, 'train_steps_per_second': 0.797, 'total_flos': 2045896481046528.0, 'train_loss': 2.455147182009348, 'epoch': 1.0, 'step': 486}\n"
     ]
    }
   ],
   "source": [
    "#########################_Проверка_мол_все_ли_у_нас_хорошо_#########################\n",
    "\n",
    "if 'model_to_train' not in globals() or not model_to_train:\n",
    "    print(\"Warning: Модель для обучения model_to_train не определена, ищи ошибку\")\n",
    "elif 'train_dataset' not in globals() or 'eval_dataset' not in globals():\n",
    "    print(\"Warnin: Обучающий или валидационный датасет не готовы\")\n",
    "elif 'tokenizer' not in globals():\n",
    "    print(\"Warnin: Токенизатор не загружен\")\n",
    "else:\n",
    "    print(\"Все компоненты для обучения готовы, красава\")\n",
    "\n",
    "    # Определяем директорию для сохранения результатов\n",
    "    output_dir = \"./results_rugpt3small_lora_qa\" # пожалуйста, пишите свою, тут моя \n",
    "\n",
    "\n",
    "#########################_____Аргументы___обучения_____#########################\n",
    "    \n",
    "    # Аргументы обучения для transformers версии ~4.41.3\n",
    "    # Учитываем ошибку ValueError: --load_best_model_at_end requires the save and eval strategy to match, мы совершенствуемся \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results_rugpt3small_lora_qa_epoch1_lr1e-4\", # Новая папка для результатов, пишите свою, тут моя \n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=1,             \n",
    "        learning_rate=1e-4,             \n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,               \n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,                 \n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,                \n",
    "        do_eval=True,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=\"none\",\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "\n",
    "#########################_Этот_блок_отвечает_за_динамическое_формирование_батчей_данных_во_время_обучения_#########################\n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "#########################_Создание_Trainer_и_обучение_#################\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model_to_train,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    print(\"\\nTrainer сконфигурирован, начинаем обучение\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        print(\"\\nОбучение завершено!\")\n",
    "\n",
    "        final_model_path = f\"{output_dir}/final_model_adapters\"\n",
    "        trainer.save_model(final_model_path)\n",
    "        print(f\"Финальные LoRA адаптеры сохранены в: {final_model_path}\")\n",
    "\n",
    "        metrics = train_result.metrics\n",
    "        print(\"\\nМетрики обучения:\")\n",
    "        print(metrics)\n",
    "        print(\"\\nЛог обучения (история):\")\n",
    "        for log_entry in trainer.state.log_history:\n",
    "            print(log_entry)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nПроизошла ошибка во время обучения: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qs3F_Nvh2NvO",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Тестирование Дообученной Модели в Colab**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puoA5zDr2Qgo"
   },
   "source": [
    "Загрузка модели с LoRA адаптерами и тестовая генерация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Lb5TojH2OpQ",
    "outputId": "d718b440-6a50-4b72-e4e2-5d0d65758b35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftConfig успешно загружен.\n",
      "Загрузка базовой модели 'sberbank-ai/rugpt3small_based_on_gpt2' с квантованием для инференса...\n",
      "Базовая модель успешно загружена с квантованием.\n",
      "Загрузка модели с LoRA адаптерами из ./results_rugpt3small_lora_qa/final_model_adapters...\n",
      "Модель с LoRA адаптерами готова для инференса.\n",
      "\n",
      "--- Тестирование модели ---\n",
      "\n",
      "Вопрос: Кто по образованию Балакина Татьяна Петровна?\n",
      "Ответ модели: Ответ: Ответ: Балакин Татьяна Петровна — специалист по экономике и финансам, экономист, аналитик, аналитик. Она преподаёт в средней школе, и она преподаёт по специальности «Экономика и финансы», в том числе по специальности «Экономика и финансы», в которой разбирается в работе с бюджетом и финансовой аналитикой. Она преподаёт в МГУ, но её специальность не преподаёт в МГУ, в которой разбирается в экономике и финансовой аналитике, а также в том числе в финансовой\n",
      "\n",
      "Вопрос: Какие направления в образовании упомянуты в контексте обсуждения с Татьяной Петровной?\n",
      "Ответ модели: Ответ:\n",
      "1. Поступление в институт, сдача ЕГЭ, сдача ЕГЭ, сдача ГИА, сдача ЕГЭ, сдача ЕГЭ, сдача ЕГЭ.\n",
      "2. Поступление в аспирантуру, сдача ЕГЭ, сдача ГИА, сдача ГИА, сдача ЕГЭ, сдача ЕГЭ, сдача ЕГЭ, сдача ГИА, сдача ЕГЭ, сдача ЕГЭ, сдача ЕГЭ, сдача ЕГЭ, сдача ЕГЭ, сдача ЕГЭ, сда\n",
      "\n",
      "Вопрос: Почему важно участвовать в олимпиадах и хакатонах по мнению Татьяны Петровны?\n",
      "Ответ модели: Участие в олимпиадах и хакатонах по мнению Татьяны Петровны важно для развития карьеры, потому что они способствуют развитию навыков и навыков работы в команде, облегчают понимание того, как работать в команде, облегчают понимание принципов построения алгоритмов и позволяют глубже понимать и принимать решения, облегчают понимание алгоритмов и дают понимание того, как работать с большими командами, облегчают понимание и принятие решений в команде, облегчают понимание и принятие решений в команде, облегчают понимание и принятие решений в команде\n"
     ]
    }
   ],
   "source": [
    "#########################_Проверка_мол_все_ли_у_нас_хорошо_#########################\n",
    "\n",
    "if 'model_name' not in globals(): \n",
    "    model_name = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
    "if 'tokenizer' not in globals() or tokenizer.name_or_path != model_name:\n",
    "    print(f\"Загрузка токенизатора для {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"Установлен pad_token: {tokenizer.pad_token}\")\n",
    "\n",
    "\n",
    "# Путь к сохраненным LoRA адаптерам\n",
    "lora_adapters_path = \"./results_rugpt3small_lora_qa/final_model_adapters\" # схему выучили?\n",
    "\n",
    "#########################_Загрузка_конфигурации_PEFT_LoRA_#########################\n",
    "\n",
    "try:\n",
    "    config = PeftConfig.from_pretrained(lora_adapters_path)\n",
    "    print(\"PeftConfig успешно загружен.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при загрузке PeftConfig: {e}. Проверьте путь: {lora_adapters_path}\")\n",
    "    config = None\n",
    "\n",
    "\n",
    "#########################_Загрузка_базовой_модели_(неквантованной/квантованной, как при обучении)#########################\n",
    "if config:\n",
    "\n",
    "    # Для инференса мы можем загрузить ее без BitsAndBytesConfig, если хотим максимального качества\n",
    "    # или с BitsAndBytesConfig, если хотим сохранить экономию памяти\n",
    "    # Летс, чек с BitsAndBytesConfig для консистентности и экономии\n",
    "\n",
    "    bnb_config_inference = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, # Или load_in_8bit=True\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16, # для T4 обычно хорошо\n",
    "        bnb_4bit_use_double_quant=False, # Можно попробовать True для большей экономии\n",
    "    )\n",
    "\n",
    "    print(f\"Загрузка базовой модели '{config.base_model_name_or_path}' с квантованием для инференса\")\n",
    "    try:\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.base_model_name_or_path, \n",
    "            quantization_config=bnb_config_inference, # Применяем квантование\n",
    "            device_map=\"auto\", \n",
    "            trust_remote_code=True \n",
    "        )\n",
    "        print(\"Базовая модель успешно загружена с квантованием, красава\")\n",
    "\n",
    "\n",
    "#########################_Слияние\" LoRA адаптеров с базовой моделью_#########################\n",
    "        print(f\"Загрузка модели с LoRA адаптерами из {lora_adapters_path}...\")\n",
    "        inference_model = PeftModel.from_pretrained(base_model, lora_adapters_path)\n",
    "\n",
    "        # Если мы хотим получить \"слитую\" модель (где веса LoRA объединены с базовыми)\n",
    "        # и использовать ее как обычную модель transformers (это требует больше памяти)\n",
    "        # merged_model = inference_model.merge_and_unload()\n",
    "        # print(\"Модель слита (LoRA адаптеры интегрированы в базовую модель).\")\n",
    "        # final_model_for_inference = merged_model\n",
    "\n",
    "        # Пока будем использовать PeftModel напрямую, это более экономно по памяти\n",
    "        final_model_for_inference = inference_model\n",
    "        final_model_for_inference.eval() # Переводим модель в режим оценки\n",
    "\n",
    "        print(\"Модель с LoRA адаптерами готова для инференса, красава\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке базовой модели или применении LoRA: {e}\")\n",
    "        final_model_for_inference = None\n",
    "else:\n",
    "    final_model_for_inference = None\n",
    "\n",
    "#########################_Функция_для_генерации_ответа_#########################\n",
    "if final_model_for_inference and tokenizer:\n",
    "    def generate_answer(question, model, tokenizer, max_new_tokens=100, temperature=0.7, top_p=0.9):\n",
    "        # Формируем промпт так же, как при обучении (но без ответа)\n",
    "        prompt = f\"Инструкция: {question}\\n\\nОтвет:\"\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512-max_new_tokens) \n",
    "        inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}\n",
    "\n",
    "\n",
    "        # Генерация ответа\n",
    "        with torch.no_grad(): \n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,      \n",
    "                temperature=temperature,            \n",
    "                top_p=top_p,                        \n",
    "                # num_beams=1,                      \n",
    "                do_sample=True,                     \n",
    "                pad_token_id=tokenizer.pad_token_id, \n",
    "                eos_token_id=tokenizer.eos_token_id  \n",
    "            )\n",
    "\n",
    "        # Декодируем сгенерированные токены в текст (Мы берем только сгенерированную часть, исключая промпт)\n",
    "        answer_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        answer_text = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "        return answer_text.strip()\n",
    "\n",
    "    # Тестовые запросы, взял из датасета output.jsonl\n",
    "    test_questions = [\n",
    "        \"Кто по образованию Балакина Татьяна Петровна?\", \n",
    "        \"Какие направления в образовании упомянуты в контексте обсуждения с Татьяной Петровной?\", \n",
    "        \"Почему важно участвовать в олимпиадах и хакатонах по мнению Татьяны Петровны?\" \n",
    "\n",
    "    print(\"\\n---###_____Тестирование_модели_____###---\")\n",
    "    for q in test_questions:\n",
    "        print(f\"\\nВопрос: {q}\")\n",
    "        answer = generate_answer(q, final_model_for_inference, tokenizer)\n",
    "        print(f\"Ответ модели: {answer}\")\n",
    "\n",
    "else:\n",
    "    print(\"Модель для инференса не была загружена, ищи ошибки\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 917
    },
    "id": "NWOFIRf37qdR",
    "outputId": "3e7ce52a-62c7-4e8c-e44a-8d184c397bce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-6bd1701e863d>:62: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Все компоненты для обучения готовы.\n",
      "Результаты будут сохранены в: ./results_rugpt3small_lora_qa_epoch1_lr2e-4\n",
      "\n",
      "Trainer сконфигурирован. Начинаем обучение...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='486' max='486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [486/486 10:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.818100</td>\n",
       "      <td>3.149058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.933600</td>\n",
       "      <td>2.654019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.770200</td>\n",
       "      <td>2.586972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.714500</td>\n",
       "      <td>2.563312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Обучение завершено!\n",
      "Финальные LoRA адаптеры сохранены в: ./results_rugpt3small_lora_qa_epoch1_lr2e-4/final_lora_adapters\n",
      "\n",
      "Метрики обучения:\n",
      "{'train_runtime': 610.9458, 'train_samples_per_second': 12.728, 'train_steps_per_second': 0.795, 'total_flos': 2045896481046528.0, 'train_loss': 3.296465383145054, 'epoch': 1.0}\n",
      "\n",
      "Лог обучения (история):\n",
      "{'loss': 6.4356, 'grad_norm': 2.951761245727539, 'learning_rate': 0.00018065843621399178, 'epoch': 0.102880658436214, 'step': 50}\n",
      "{'loss': 3.8181, 'grad_norm': 2.39199161529541, 'learning_rate': 0.00016008230452674898, 'epoch': 0.205761316872428, 'step': 100}\n",
      "{'eval_loss': 3.1490583419799805, 'eval_runtime': 18.1111, 'eval_samples_per_second': 47.761, 'eval_steps_per_second': 6.018, 'epoch': 0.205761316872428, 'step': 100}\n",
      "{'loss': 3.1948, 'grad_norm': 1.6853724718093872, 'learning_rate': 0.00013950617283950618, 'epoch': 0.30864197530864196, 'step': 150}\n",
      "{'loss': 2.9336, 'grad_norm': 1.2697535753250122, 'learning_rate': 0.00011893004115226339, 'epoch': 0.411522633744856, 'step': 200}\n",
      "{'eval_loss': 2.6540191173553467, 'eval_runtime': 18.2137, 'eval_samples_per_second': 47.492, 'eval_steps_per_second': 5.984, 'epoch': 0.411522633744856, 'step': 200}\n",
      "{'loss': 2.8116, 'grad_norm': 1.251987099647522, 'learning_rate': 9.835390946502057e-05, 'epoch': 0.51440329218107, 'step': 250}\n",
      "{'loss': 2.7702, 'grad_norm': 1.0519262552261353, 'learning_rate': 7.777777777777778e-05, 'epoch': 0.6172839506172839, 'step': 300}\n",
      "{'eval_loss': 2.5869715213775635, 'eval_runtime': 18.1954, 'eval_samples_per_second': 47.539, 'eval_steps_per_second': 5.991, 'epoch': 0.6172839506172839, 'step': 300}\n",
      "{'loss': 2.7434, 'grad_norm': 1.3491405248641968, 'learning_rate': 5.720164609053498e-05, 'epoch': 0.720164609053498, 'step': 350}\n",
      "{'loss': 2.7145, 'grad_norm': 1.3534064292907715, 'learning_rate': 3.662551440329218e-05, 'epoch': 0.823045267489712, 'step': 400}\n",
      "{'eval_loss': 2.563311815261841, 'eval_runtime': 18.1815, 'eval_samples_per_second': 47.576, 'eval_steps_per_second': 5.995, 'epoch': 0.823045267489712, 'step': 400}\n",
      "{'loss': 2.6754, 'grad_norm': 1.1029260158538818, 'learning_rate': 1.604938271604938e-05, 'epoch': 0.9259259259259259, 'step': 450}\n",
      "{'train_runtime': 610.9458, 'train_samples_per_second': 12.728, 'train_steps_per_second': 0.795, 'total_flos': 2045896481046528.0, 'train_loss': 3.296465383145054, 'epoch': 1.0, 'step': 486}\n"
     ]
    }
   ],
   "source": [
    "#########################_Проверка_мол_все_ли_у_нас_хорошо_#########################\n",
    "\n",
    "if 'model_to_train' not in globals() or not model_to_train:\n",
    "    print(\"WARNING: Модель для обучения model_to_train не определена\")\n",
    "    raise RuntimeError(\"model_to_train не определена, Прерываем принудительно\")\n",
    "elif 'train_dataset' not in globals() or 'eval_dataset' not in globals():\n",
    "    print(\"WARNING: train/val датасет не готовы\")\n",
    "    raise RuntimeError(\"train_dataset или eval_dataset не определены, Прерываем принудительно\")\n",
    "\n",
    "elif 'tokenizer' not in globals():\n",
    "    print(\"WARNING: Токенизатор не загружен\")\n",
    "    raise RuntimeError(\"tokenizer не определен, Прерываем принудительно\")\n",
    "else:\n",
    "    print(\"Все компоненты для обучения готовы\")\n",
    "\n",
    "    current_output_dir = \"./results_rugpt3small_lora_qa_epoch1_lr2e-4\" # пожалуйста, указывайте свой путь \n",
    "    print(f\"Результаты будут сохранены в: {current_output_dir}\")\n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=current_output_dir,      \n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=1,                 \n",
    "        learning_rate=2e-4,                 \n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,                   \n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,                     \n",
    "        eval_strategy=\"steps\",              \n",
    "        eval_steps=100,                     \n",
    "        do_eval=True,                       \n",
    "        save_total_limit=2,                 \n",
    "        load_best_model_at_end=True,        \n",
    "        metric_for_best_model=\"eval_loss\",  \n",
    "        greater_is_better=False,            \n",
    "        report_to=\"none\",                   \n",
    "        fp16=True,                          \n",
    "\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # Создание Trainer\n",
    "\n",
    "    if not hasattr(model_to_train, 'peft_config'):\n",
    "        print(\"ПРЕДУПРЕЖДЕНИЕ: 'model_to_train' не выглядит как PEFT модель.\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model_to_train,              \n",
    "        args=training_args,                 \n",
    "        train_dataset=train_dataset,        \n",
    "        eval_dataset=eval_dataset,          \n",
    "        tokenizer=tokenizer,                \n",
    "        data_collator=data_collator,        \n",
    "    )\n",
    "\n",
    "#########################_Обучение_#########################\n",
    "\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        print(\"\\nОбучение завершено!\")\n",
    "        final_lora_adapters_path = f\"{current_output_dir}/final_lora_adapters\"\n",
    "        trainer.save_model(final_lora_adapters_path)\n",
    "        print(f\"Финальные LoRA адаптеры сохранены в: {final_lora_adapters_path}\")\n",
    "\n",
    "        metrics = train_result.metrics\n",
    "        print(\"\\nМетрики обучения:\")\n",
    "        print(metrics)\n",
    "        print(\"\\nЛог обучения (история):\")\n",
    "        for log_entry in trainer.state.log_history:\n",
    "            print(log_entry)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nПроизошла ошибка во время обучения, переделывай: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGrIeKcY-vBV",
    "outputId": "43b8760d-1de8-44f1-f36d-6b2910226a3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка токенизатора для: sberbank-ai/rugpt3small_based_on_gpt2\n",
      "\n",
      "Загрузка PeftConfig из: ./results_rugpt3small_lora_qa_epoch1_lr2e-4/final_lora_adapters\n",
      "PeftConfig успешно загружен.\n",
      "\n",
      "Загрузка базовой модели 'sberbank-ai/rugpt3small_based_on_gpt2' с квантованием для инференса...\n",
      "Базовая модель успешно загружена с квантованием.\n",
      "Загрузка и применение LoRA адаптеров из ./results_rugpt3small_lora_qa_epoch1_lr2e-4/final_lora_adapters к базовой модели...\n",
      "Модель с LoRA адаптерами готова для инференса.\n",
      "\n",
      "--- Тестирование модели ---\n",
      "Используются адаптеры из: ./results_rugpt3small_lora_qa_epoch1_lr2e-4/final_lora_adapters\n",
      "Параметры генерации: max_new_tokens=150, temperature=0.6, top_p=0.9\n",
      "\n",
      "Вопрос: Кто по образованию Балакина Татьяна Петровна?\n",
      "Ответ модели: Балакина Татьяна Петровна — преподаватель, кандидат филологических наук, автор многочисленных публикаций, в том числе нескольких монографий.\n",
      "\n",
      "Вопрос: Какие направления в образовании упомянуты в контексте обсуждения с Татьяной Петровной?\n",
      "Ответ модели: В контексте обсуждения с Татьяной Петровной упомянуты следующие направления в образовании упомянуты следующие направления в образовании упомянутых направлений:\n",
      "\n",
      "1) Образование и экономика\n",
      "2) Математика\n",
      "3) Экономика и бизнес\n",
      "4) Менеджмент\n",
      "5) Экономика и бизнес-информатика\n",
      "6) Экономика и бизнес-информатика\n",
      "7) Социальная работа\n",
      "8) Математика и экономика\n",
      "9) Экономика и бизнес\n",
      "10) Математика и бизнес-информатика\n",
      "11) Экономика и бизнес-информатика\n",
      "11) Экономика и бизнес-информатика\n",
      "11) Экономика и бизнес-информатика\n",
      "12) Экономика и бизнес-информатика\n",
      "13) Математика и бизнес-\n",
      "\n",
      "Вопрос: Почему важно участвовать в олимпиадах и хакатонах по мнению Татьяны Петровны?\n",
      "Ответ модели: Участие в олимпиадах и хакатонах по мнению Татьяны Петровны важно, потому что это позволяет лучше понять специфику проведения олимпиад и хакатонов по мнению студентов. Ответ: Участие в олимпиадах и хакатонах по мнению Татьяны Петровны является важным для студентов, поскольку позволяет лучше понять специфику проведения олимпиад и хакатонов по мнению студентов, которые хотят получить знания по теме.\n"
     ]
    }
   ],
   "source": [
    "lora_adapters_path = \"./results_rugpt3small_lora_qa_epoch1_lr2e-4/final_lora_adapters\"\n",
    "base_model_name_for_inference = \"sberbank-ai/rugpt3small_based_on_gpt2\" \n",
    "\n",
    "# Параметры генерации\n",
    "generation_max_new_tokens = 150\n",
    "generation_temperature = 0.6 # Можно начать с этого, потом экспериментировать\n",
    "generation_top_p = 0.9\n",
    "\n",
    "print(f\"Загрузка токенизатора для: {base_model_name_for_inference}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name_for_inference)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"Установлен pad_token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при загрузке токенизатора: {e}\")\n",
    "    tokenizer = None\n",
    "\n",
    "\n",
    "#########################_Загрузка_конфигурации_PEFT_(LoRA)_#########################\n",
    "\n",
    "if tokenizer:\n",
    "    try:\n",
    "        print(f\"\\nЗагрузка PeftConfig из: {lora_adapters_path}\")\n",
    "        config = PeftConfig.from_pretrained(lora_adapters_path)\n",
    "        if config.base_model_name_or_path != base_model_name_for_inference:\n",
    "            print(f\"ПРЕДУПРЕЖДЕНИЕ: Имя базовой модели в PeftConfig ({config.base_model_name_or_path}) \"\n",
    "                  f\"не совпадает с ожидаемым ({base_model_name_for_inference}). Используем имя из PeftConfig.\")\n",
    "        actual_base_model_name = config.base_model_name_or_path\n",
    "        print(\"PeftConfig успешно загружен\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке PeftConfig: {e}. Проверьте путь: {lora_adapters_path}\")\n",
    "        config = None\n",
    "        actual_base_model_name = None\n",
    "else:\n",
    "    config = None\n",
    "    actual_base_model_name = None\n",
    "\n",
    "final_model_for_inference = None\n",
    "if config and actual_base_model_name:\n",
    "\n",
    "#########################_Загрузка базовой модели с квантованием_#########################\n",
    "    \n",
    "    bnb_config_inference = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nЗагрузка базовой модели '{actual_base_model_name}' с квантованием для инференса...\")\n",
    "    try:\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            actual_base_model_name,\n",
    "            quantization_config=bnb_config_inference,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(\"Базовая модель успешно загружена с квантованием\")\n",
    "\n",
    "#########################_Применение LoRA адаптеров_#########################\n",
    "        \n",
    "        print(f\"Загрузка и применение LoRA адаптеров из {lora_adapters_path} к базовой модели...\")\n",
    "        final_model_for_inference = PeftModel.from_pretrained(base_model, lora_adapters_path)\n",
    "        final_model_for_inference.eval() # Переводим модель в режим оценки\n",
    "        print(\"Модель с LoRA адаптерами готова для инференса\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке базовой модели или применении LoRA: {e}\")\n",
    "        final_model_for_inference = None\n",
    "else:\n",
    "    if not tokenizer:\n",
    "        print(\"Токенизатор не был загружен, инференс невозможен.\")\n",
    "    elif not config:\n",
    "         print(\"PeftConfig не был загружен, инференс невозможен.\")\n",
    "    elif not actual_base_model_name:\n",
    "        print(\"Не удалось определить имя базовой модели из PeftConfig.\")\n",
    "\n",
    "#########################_Функция_для_генерации_ответа__#########################\n",
    "if final_model_for_inference and tokenizer:\n",
    "    def generate_answer(question, model, tokenizer, max_new_tokens, temperature, top_p):\n",
    "        prompt = f\"Инструкция: {question}\\n\\nОтвет:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512 - max_new_tokens)\n",
    "        inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        answer_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        answer_text = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "        # Постобработка для удаления лишних \"Ответ:\"\n",
    "        processed_answer = answer_text.strip()\n",
    "        while processed_answer.lower().startswith(\"ответ:\"):\n",
    "            processed_answer = processed_answer[len(\"ответ:\"):]\n",
    "            processed_answer = processed_answer.strip()\n",
    "\n",
    "        return processed_answer\n",
    "\n",
    "    # Тестовые запросы\n",
    "    test_questions = [\n",
    "        \"Кто по образованию Балакина Татьяна Петровна?\",\n",
    "        \"Какие направления в образовании упомянуты в контексте обсуждения с Татьяной Петровной?\",\n",
    "        \"Почему важно участвовать в олимпиадах и хакатонах по мнению Татьяны Петровны?\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\n--- Тестирование модели ---\")\n",
    "    print(f\"Используются адаптеры из: {lora_adapters_path}\")\n",
    "    print(f\"Параметры генерации: max_new_tokens={generation_max_new_tokens}, temperature={generation_temperature}, top_p={generation_top_p}\")\n",
    "    for q in test_questions:\n",
    "        print(f\"\\nВопрос: {q}\")\n",
    "        answer = generate_answer(q, final_model_for_inference, tokenizer,\n",
    "                                 generation_max_new_tokens, generation_temperature, generation_top_p)\n",
    "        print(f\"Ответ модели: {answer}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nМодель для инференса не была загружена или токенизатор отсутствует. Ищи ошибки.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Переобучение с lr=1e-4, 1 эпоха, с сохранением в подпапку**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "id": "DqaWbOFRAxas",
    "outputId": "3cc86ab6-4772-424c-8c23-9475e2edede0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-e21b97bdab94>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Все компоненты для обучения готовы.\n",
      "Результаты (чекпоинты) будут в: ./results_rugpt3small_lora_qa_epoch1_lr1e-4_v2\n",
      "Финальные адаптеры будут сохранены в: ./results_rugpt3small_lora_qa_epoch1_lr1e-4_v2/final_lora_adapters\n",
      "\n",
      "Trainer сконфигурирован. Начинаем обучение...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='486' max='486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [486/486 10:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.702000</td>\n",
       "      <td>2.531617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.655200</td>\n",
       "      <td>2.510149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.632900</td>\n",
       "      <td>2.496098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.618900</td>\n",
       "      <td>2.489911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Обучение завершено!\n",
      "Финальные LoRA адаптеры сохранены в: ./results_rugpt3small_lora_qa_epoch1_lr1e-4_v2/final_lora_adapters\n",
      "\n",
      "Метрики обучения:\n",
      "{'train_runtime': 612.0398, 'train_samples_per_second': 12.705, 'train_steps_per_second': 0.794, 'total_flos': 2045896481046528.0, 'train_loss': 2.6483464182159047, 'epoch': 1.0}\n",
      "\n",
      "Лог обучения (история):\n",
      "{'loss': 2.7115, 'grad_norm': 1.3129390478134155, 'learning_rate': 8.991769547325102e-05, 'epoch': 0.102880658436214, 'step': 50}\n",
      "{'loss': 2.702, 'grad_norm': 1.419616937637329, 'learning_rate': 7.962962962962964e-05, 'epoch': 0.205761316872428, 'step': 100}\n",
      "{'eval_loss': 2.5316171646118164, 'eval_runtime': 18.2369, 'eval_samples_per_second': 47.431, 'eval_steps_per_second': 5.977, 'epoch': 0.205761316872428, 'step': 100}\n",
      "{'loss': 2.6608, 'grad_norm': 1.4007493257522583, 'learning_rate': 6.934156378600823e-05, 'epoch': 0.30864197530864196, 'step': 150}\n",
      "{'loss': 2.6552, 'grad_norm': 1.2709611654281616, 'learning_rate': 5.905349794238684e-05, 'epoch': 0.411522633744856, 'step': 200}\n",
      "{'eval_loss': 2.5101490020751953, 'eval_runtime': 18.2332, 'eval_samples_per_second': 47.441, 'eval_steps_per_second': 5.978, 'epoch': 0.411522633744856, 'step': 200}\n",
      "{'loss': 2.639, 'grad_norm': 1.1863102912902832, 'learning_rate': 4.876543209876544e-05, 'epoch': 0.51440329218107, 'step': 250}\n",
      "{'loss': 2.6329, 'grad_norm': 1.2066991329193115, 'learning_rate': 3.8477366255144036e-05, 'epoch': 0.6172839506172839, 'step': 300}\n",
      "{'eval_loss': 2.4960978031158447, 'eval_runtime': 18.1963, 'eval_samples_per_second': 47.537, 'eval_steps_per_second': 5.99, 'epoch': 0.6172839506172839, 'step': 300}\n",
      "{'loss': 2.6317, 'grad_norm': 1.4663808345794678, 'learning_rate': 2.8189300411522634e-05, 'epoch': 0.720164609053498, 'step': 350}\n",
      "{'loss': 2.6189, 'grad_norm': 1.2663116455078125, 'learning_rate': 1.7901234567901236e-05, 'epoch': 0.823045267489712, 'step': 400}\n",
      "{'eval_loss': 2.4899113178253174, 'eval_runtime': 18.1976, 'eval_samples_per_second': 47.534, 'eval_steps_per_second': 5.99, 'epoch': 0.823045267489712, 'step': 400}\n",
      "{'loss': 2.5988, 'grad_norm': 1.1318612098693848, 'learning_rate': 7.613168724279836e-06, 'epoch': 0.9259259259259259, 'step': 450}\n",
      "{'train_runtime': 612.0398, 'train_samples_per_second': 12.705, 'train_steps_per_second': 0.794, 'total_flos': 2045896481046528.0, 'train_loss': 2.6483464182159047, 'epoch': 1.0, 'step': 486}\n"
     ]
    }
   ],
   "source": [
    "#########################_Проверка_мол_все_ли_у_нас_хорошо_#########################\n",
    "\n",
    "if 'model_to_train' not in globals() or not model_to_train:\n",
    "    print(\"WARNING: Модель для обучения model_to_train не определена\")\n",
    "    raise RuntimeError(\"model_to_train не определена, принудительное прерывание.\")\n",
    "elif 'train_dataset' not in globals() or 'eval_dataset' not in globals():\n",
    "    print(\"WARNING: train/val датасет не готовы\")\n",
    "    raise RuntimeError(\"train_dataset или eval_dataset не определены, принудительное прерывание.\")\n",
    "elif 'tokenizer' not in globals():\n",
    "    print(\"WARNING: Токенизатор не загружен\")\n",
    "    raise RuntimeError(\"tokenizer не определен. принудительное прерывание\")\n",
    "else:\n",
    "    print(\"Все компоненты для обучения готовы.\")\n",
    "\n",
    "    base_output_dir = \"./results_rugpt3small_lora_qa_epoch1_lr1e-4_v2\" # Добавил _v2 для уникальности\n",
    "    # Путь для сохранения финальных адаптеров внутри base_output_dir\n",
    "    final_lora_adapters_path = os.path.join(base_output_dir, \"final_lora_adapters\")\n",
    "\n",
    "    print(f\"Результаты (чекпоинты) будут в: {base_output_dir}\")\n",
    "    print(f\"Финальные адаптеры будут сохранены в: {final_lora_adapters_path}\")\n",
    "\n",
    "    # Аргументы обучения\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=base_output_dir,         # Чекпоинты будут сохраняться сюда\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=1,                 \n",
    "        learning_rate=1e-4,                 \n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        do_eval=True,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=\"none\",\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    if not hasattr(model_to_train, 'peft_config'):\n",
    "        print(\"ПРЕДУПРЕЖДЕНИЕ: 'model_to_train' не выглядит как PEFT модель.\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model_to_train,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    print(\"\\nTrainer сконфигурирован. Начинаем обучение...\")\n",
    "\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        print(\"\\nОбучение завершено!\")\n",
    "\n",
    "        # Явно создаем директорию для финальных адаптеров, если ее нет\n",
    "        os.makedirs(final_lora_adapters_path, exist_ok=True)\n",
    "        trainer.save_model(final_lora_adapters_path)\n",
    "        print(f\"Финальные LoRA адаптеры сохранены в: {final_lora_adapters_path}\")\n",
    "\n",
    "        # tokenizer.save_pretrained(final_lora_adapters_path) # Если нужно\n",
    "\n",
    "        metrics = train_result.metrics\n",
    "        print(\"\\nМетрики обучения:\")\n",
    "        print(metrics)\n",
    "        print(\"\\nЛог обучения (история):\")\n",
    "        for log_entry in trainer.state.log_history:\n",
    "            print(log_entry)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nПроизошла ошибка во время обучения: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Новый блок, чтобы не заплывало в глазах, новый тест**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G7bhQDaSA3vQ",
    "outputId": "ae1b7eee-b38e-4a81-845f-363be0dba6c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка токенизатора для: sberbank-ai/rugpt3small_based_on_gpt2\n",
      "\n",
      "Загрузка PeftConfig из: /content/results_rugpt3small_lora_qa_epoch1_lr1e-4_v2/final_lora_adapters\n",
      "PeftConfig успешно загружен.\n",
      "\n",
      "Загрузка базовой модели 'sberbank-ai/rugpt3small_based_on_gpt2' с квантованием для инференса...\n",
      "Базовая модель успешно загружена с квантованием.\n",
      "Загрузка и применение LoRA адаптеров из /content/results_rugpt3small_lora_qa_epoch1_lr1e-4_v2/final_lora_adapters к базовой модели...\n",
      "Модель с LoRA адаптерами готова для инференса.\n",
      "\n",
      "--- Тестирование модели ---\n",
      "Используются адаптеры из: /content/results_rugpt3small_lora_qa_epoch1_lr1e-4_v2/final_lora_adapters\n",
      "Используемые параметры генерации: {'max_new_tokens': 150, 'temperature': 0.1, 'top_p': 0.9, 'repetition_penalty': 1.15, 'do_sample': True}\n",
      "\n",
      "Вопрос: Кто по образованию Балакина Татьяна Петровна?\n",
      "Ответ модели: По образованию Балакин Дмитрий Петрович, который учился в школе № 1.\n",
      "\n",
      "Вопрос: Какие направления в образовании упомянуты в контексте обсуждения с Татьяной Петровной?\n",
      "Ответ модели: В обсуждении с Татьяной Петровной упоминаются следующие направления в образовании, которые связаны с развитием и продвижением науки.\n",
      "* * *\n",
      "* * *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "* *\n",
      "\n",
      "Вопрос: Почему важно участвовать в олимпиадах и хакатонах по мнению Татьяны Петровны?\n",
      "Ответ модели: Участие в олимпиадах и хакатонах по мнению Татьяны Петровны — это возможность получить знания, которые помогут лучше понять себя. Это помогает глубже понимать свои задачи, а также повысить мотивацию к обучению. Также участие в олимпиадах даёт возможность улучшить навыки работы с информацией. Кроме того, участие в олимпиадах позволяет более эффективно использовать полученные знания для развития навыков и повышения мотивации к учёбе. Мнение о том, что нужно учиться на чужих ошибках полезно для карьеры, не является обязательным условием участия в олимпиадах и хакатонах.\n"
     ]
    }
   ],
   "source": [
    "lora_adapters_path = \"/content/results_rugpt3small_lora_qa_epoch1_lr1e-4_v2/final_lora_adapters\"\n",
    "\n",
    "\n",
    "base_model_name_for_inference = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
    "\n",
    "# Набор 1: С repetition_penalty и умеренной температурой \n",
    "generation_params_set = {\n",
    "    \"max_new_tokens\": 150,\n",
    "    \"temperature\": 0.1, # Можно варьировать 0.2 - 0.7\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.15,\n",
    "    \"do_sample\": True\n",
    "}\n",
    "\n",
    "# Набор 2: Beam Search (альтернатива)\n",
    "# generation_params_set = {\n",
    "#     \"max_new_tokens\": 150,\n",
    "#     \"num_beams\": 3,\n",
    "#     \"early_stopping\": True,\n",
    "#     \"repetition_penalty\": 1.1,\n",
    "#     \"do_sample\": False\n",
    "# }\n",
    "\n",
    "current_generation_params = generation_params_set\n",
    "print(f\"Загрузка токенизатора для: {base_model_name_for_inference}\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name_for_inference)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"Установлен pad_token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при загрузке токенизатора: {e}\")\n",
    "    tokenizer = None\n",
    "\n",
    "if not os.path.exists(lora_adapters_path):\n",
    "    print(f\"ОШИБКА: Путь к адаптерам не найден: {lora_adapters_path}\")\n",
    "    print(\"Пожалуйста, проверьте переменную 'lora_adapters_path' и убедитесь, что адаптеры существуют по этому пути\")\n",
    "    config = None\n",
    "    actual_base_model_name = None\n",
    "elif tokenizer:\n",
    "    try:\n",
    "        print(f\"\\nЗагрузка PeftConfig из: {lora_adapters_path}\")\n",
    "        config = PeftConfig.from_pretrained(lora_adapters_path)\n",
    "        if config.base_model_name_or_path.lower() != base_model_name_for_inference.lower():\n",
    "            print(f\"ПРЕДУПРЕЖДЕНИЕ: Имя базовой модели в PeftConfig ({config.base_model_name_or_path}) \"\n",
    "                  f\"не совпадает с ожидаемым ({base_model_name_for_inference}). Используем имя из PeftConfig.\")\n",
    "        actual_base_model_name = config.base_model_name_or_path\n",
    "        print(\"PeftConfig успешно загружен.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке PeftConfig: {e}\")\n",
    "        config = None\n",
    "        actual_base_model_name = None\n",
    "else:\n",
    "    config = None\n",
    "    actual_base_model_name = None\n",
    "\n",
    "\n",
    "final_model_for_inference = None\n",
    "if config and actual_base_model_name:\n",
    "    bnb_config_inference = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nЗагрузка базовой модели '{actual_base_model_name}' с квантованием для инференса...\")\n",
    "    try:\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            actual_base_model_name,\n",
    "            quantization_config=bnb_config_inference,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(\"Базовая модель успешно загружена с квантованием.\")\n",
    "\n",
    "        print(f\"Загрузка и применение LoRA адаптеров из {lora_adapters_path} к базовой модели...\")\n",
    "        final_model_for_inference = PeftModel.from_pretrained(base_model, lora_adapters_path)\n",
    "        final_model_for_inference.eval()\n",
    "        print(\"Модель с LoRA адаптерами готова для инференса.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке базовой модели или применении LoRA: {e}\")\n",
    "        final_model_for_inference = None\n",
    "else:\n",
    "    if not tokenizer:\n",
    "        print(\"Токенизатор не был загружен\")\n",
    "    elif not os.path.exists(lora_adapters_path):\n",
    "        pass\n",
    "    elif not config:\n",
    "         print(\"PeftConfig не был загружен\")\n",
    "    elif not actual_base_model_name:\n",
    "        print(\"Не удалось определить имя базовой модели из PeftConfig.\")\n",
    "\n",
    "\n",
    "if final_model_for_inference and tokenizer:\n",
    "    def generate_answer(question, model, tokenizer, gen_params):\n",
    "        prompt = f\"Инструкция: {question}\\n\\nОтвет:\"\n",
    "        max_input_length = 512 - gen_params.get(\"max_new_tokens\", 100)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_length)\n",
    "        inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}\n",
    "\n",
    "        generation_call_params = {**inputs, **gen_params}\n",
    "        generation_call_params['pad_token_id'] = tokenizer.pad_token_id\n",
    "        generation_call_params['eos_token_id'] = tokenizer.eos_token_id\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**generation_call_params)\n",
    "\n",
    "        answer_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        answer_text = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "        processed_answer = answer_text.strip()\n",
    "        while processed_answer.lower().startswith(\"ответ:\"):\n",
    "            processed_answer = processed_answer[len(\"ответ:\"):]\n",
    "            processed_answer = processed_answer.strip()\n",
    "\n",
    "        return processed_answer\n",
    "\n",
    "    test_questions = [\n",
    "        \"Кто по образованию Балакина Татьяна Петровна?\",\n",
    "        \"Какие направления в образовании упомянуты в контексте обсуждения с Татьяной Петровной?\",\n",
    "        \"Почему важно участвовать в олимпиадах и хакатонах по мнению Татьяны Петровны?\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\n--- Тестирование модели ---\")\n",
    "    print(f\"Используются адаптеры из: {lora_adapters_path}\")\n",
    "    print(f\"Используемые параметры генерации: {current_generation_params}\")\n",
    "    for q in test_questions:\n",
    "        print(f\"\\nВопрос: {q}\")\n",
    "        answer = generate_answer(q, final_model_for_inference, tokenizer, current_generation_params)\n",
    "        print(f\"Ответ модели: {answer}\")\n",
    "else:\n",
    "    print(\"\\nМодель для инференса не была загружена или токенизатор отсутствует. Проверьте ошибки выше.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Конфигурация LoRA - Модифицированная**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O29Y8OCIERzA",
    "outputId": "2139dd8d-2b36-4e0d-fca0-5faf1c9bfce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Базовая модель, токенизатор и датасеты (предположительно) готовы.\n",
      "Подготовка модели для k-bit training (QLoRA)...\n",
      "Модель подготовлена для k-bit training (QLoRA).\n",
      "\n",
      "Новая LoraConfig создана: r=32, alpha=64, target_modules=['c_attn', 'c_proj', 'c_fc']\n",
      "LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=32, target_modules={'c_attn', 'c_fc', 'c_proj'}, exclude_modules=None, lora_alpha=64, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n",
      "\n",
      "Новая конфигурация LoRA применена к модели.\n",
      "\n",
      "Информация о параметрах после применения НОВОЙ LoRA:\n",
      "trainable params: 4,718,592 || all params: 129,950,208 || trainable%: 3.6311\n",
      "\n",
      "Модель 'model_to_train' готова к обучению с новой LoRA конфигурацией.\n"
     ]
    }
   ],
   "source": [
    "#########################_Проверка_мол_все_ли_у_нас_хорошо_#########################\n",
    "\n",
    "if 'model' not in globals() or 'tokenizer' not in globals():\n",
    "    print(\"WARNING: Базовая модель или токенизатор не загружены\")\n",
    "    raise RuntimeError(\"Базовая модель или токенизатор не загружены\")\n",
    "elif 'train_dataset' not in globals() or 'eval_dataset' not in globals():\n",
    "    print(\"WARNING: Обучающий или валидационный датасет не готовы\")\n",
    "else:\n",
    "    print(\"Базовая модель, токенизатор и датасеты (предположительно) готовы.\")\n",
    "\n",
    "\n",
    "#########################_Подготовка_модели_для_k-битного обучения_(если используется квантование)_#########################\n",
    "\n",
    "    try:\n",
    "        if hasattr(model, 'is_loaded_in_4bit') or hasattr(model, 'is_loaded_in_8bit'):\n",
    "             if not getattr(model, 'is_prepared_for_kbit_training', False): \n",
    "                print(\"Подготовка модели для k-bit training (QLoRA)...\")\n",
    "                model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True) # Явно включаем checkpointing\n",
    "                model.is_prepared_for_kbit_training = True # Ставим флаг\n",
    "                print(\"Модель подготовлена для k-bit training (QLoRA).\")\n",
    "             else:\n",
    "                print(\"Модель уже была подготовлена для k-bit training.\")\n",
    "        else:\n",
    "            print(\"Модель не загружена с квантованием, prepare_model_for_kbit_training не будет применен.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при prepare_model_for_kbit_training: {e}\")\n",
    "        print(\"Продолжаем без явной подготовки, но могут быть проблемы со стабильностью QLoRA, если она не была сделана ранее.\")\n",
    "\n",
    "#########################_Определение_НОВОЙ_конфигурации_LoRA_#########################\n",
    "\n",
    "    # вынес гиперпараметры \n",
    "    lora_r = 32  \n",
    "    lora_alpha = lora_r * 2 \n",
    "    lora_dropout = 0.05\n",
    "    # Для GPT-2 архитектуры (rugpt3small):\n",
    "    # 'c_attn' - attention QKV linear layer\n",
    "    # 'c_proj' - attention output projection linear layer\n",
    "    # 'c_fc'   - first linear layer in MLP\n",
    "    # 'c_proj' in MLP (второй линейный слой) тоже можно, но начнем с этих трех типов\n",
    "    lora_target_modules = [\"c_attn\", \"c_proj\", \"c_fc\"]\n",
    "\n",
    "    lora_config_new = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=lora_target_modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    print(f\"\\nНовая LoraConfig создана: r={lora_r}, alpha={lora_alpha}, target_modules={lora_target_modules}\")\n",
    "    print(lora_config_new)\n",
    "\n",
    "\n",
    "#########################_Применение_LoRA_к_модели_#########################\n",
    "    \n",
    "    # Важно: 'model' здесь должна быть базовой моделью, а не уже обернутой в PeftModel\n",
    "\n",
    "    # Проверка, не является ли 'model' уже PeftModel\n",
    "    if isinstance(model, PeftModel):\n",
    "        print(\"ПРЕДУПРЕЖДЕНИЕ: 'model' уже является PeftModel. Попытка получить базовую модель...\")\n",
    "        base_model_for_lora = model.base_model.model\n",
    "    else:\n",
    "        base_model_for_lora = model\n",
    "\n",
    "    try:\n",
    "        model_to_train = get_peft_model(base_model_for_lora, lora_config_new) \n",
    "        print(\"\\nНовая конфигурация LoRA применена к модели.\")\n",
    "\n",
    "        print(\"\\nИнформация о параметрах после применения НОВОЙ LoRA:\")\n",
    "        model_to_train.print_trainable_parameters()\n",
    "\n",
    "        print(\"\\nМодель 'model_to_train' готова к обучению с новой LoRA конфигурацией.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при применении новой LoRA к модели: {e}\")\n",
    "        model_to_train = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "id": "PvjIzGCdEdEn",
    "outputId": "ac8c2ff1-fa09-4b30-d79f-682bf2579f5a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-f92f0663e17b>:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Все компоненты для обучения (с новой LoRA конфигурацией) готовы.\n",
      "Результаты (чекпоинты) будут в: ./results_rugpt3small_lora_r32_targets_extended\n",
      "Финальные адаптеры будут сохранены в: ./results_rugpt3small_lora_r32_targets_extended/final_lora_adapters\n",
      "\n",
      "Trainer сконфигурирован. Начинаем обучение...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='486' max='486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [486/486 12:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.157800</td>\n",
       "      <td>2.635995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.648300</td>\n",
       "      <td>2.437575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.534400</td>\n",
       "      <td>2.363811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.477100</td>\n",
       "      <td>2.329300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Обучение завершено!\n",
      "Финальные LoRA адаптеры сохранены в: ./results_rugpt3small_lora_r32_targets_extended/final_lora_adapters\n",
      "\n",
      "Метрики обучения:\n",
      "{'train_runtime': 740.4349, 'train_samples_per_second': 10.502, 'train_steps_per_second': 0.656, 'total_flos': 2144523962548224.0, 'train_loss': 2.915943381227093, 'epoch': 1.0}\n",
      "\n",
      "Лог обучения (история):\n",
      "{'loss': 5.5159, 'grad_norm': 4.073153018951416, 'learning_rate': 9.012345679012346e-05, 'epoch': 0.102880658436214, 'step': 50}\n",
      "{'loss': 3.1578, 'grad_norm': 1.8507589101791382, 'learning_rate': 7.983539094650207e-05, 'epoch': 0.205761316872428, 'step': 100}\n",
      "{'eval_loss': 2.6359951496124268, 'eval_runtime': 21.1477, 'eval_samples_per_second': 40.903, 'eval_steps_per_second': 5.154, 'epoch': 0.205761316872428, 'step': 100}\n",
      "{'loss': 2.7411, 'grad_norm': 2.129037380218506, 'learning_rate': 6.954732510288067e-05, 'epoch': 0.30864197530864196, 'step': 150}\n",
      "{'loss': 2.6483, 'grad_norm': 1.7386090755462646, 'learning_rate': 5.925925925925926e-05, 'epoch': 0.411522633744856, 'step': 200}\n",
      "{'eval_loss': 2.437574625015259, 'eval_runtime': 21.3798, 'eval_samples_per_second': 40.459, 'eval_steps_per_second': 5.098, 'epoch': 0.411522633744856, 'step': 200}\n",
      "{'loss': 2.5722, 'grad_norm': 1.7888457775115967, 'learning_rate': 4.8971193415637865e-05, 'epoch': 0.51440329218107, 'step': 250}\n",
      "{'loss': 2.5344, 'grad_norm': 1.4732999801635742, 'learning_rate': 3.868312757201646e-05, 'epoch': 0.6172839506172839, 'step': 300}\n",
      "{'eval_loss': 2.3638112545013428, 'eval_runtime': 21.3453, 'eval_samples_per_second': 40.524, 'eval_steps_per_second': 5.107, 'epoch': 0.6172839506172839, 'step': 300}\n",
      "{'loss': 2.4965, 'grad_norm': 1.7733416557312012, 'learning_rate': 2.839506172839506e-05, 'epoch': 0.720164609053498, 'step': 350}\n",
      "{'loss': 2.4771, 'grad_norm': 1.478591799736023, 'learning_rate': 1.8106995884773663e-05, 'epoch': 0.823045267489712, 'step': 400}\n",
      "{'eval_loss': 2.3292999267578125, 'eval_runtime': 21.3634, 'eval_samples_per_second': 40.49, 'eval_steps_per_second': 5.102, 'epoch': 0.823045267489712, 'step': 400}\n",
      "{'loss': 2.4337, 'grad_norm': 1.4857794046401978, 'learning_rate': 7.818930041152265e-06, 'epoch': 0.9259259259259259, 'step': 450}\n",
      "{'train_runtime': 740.4349, 'train_samples_per_second': 10.502, 'train_steps_per_second': 0.656, 'total_flos': 2144523962548224.0, 'train_loss': 2.915943381227093, 'epoch': 1.0, 'step': 486}\n"
     ]
    }
   ],
   "source": [
    "#########################_Проверка_мол_все_ли_у_нас_хорошо_#########################\n",
    "\n",
    "if 'model_to_train' not in globals() or not model_to_train or not hasattr(model_to_train, 'peft_config'):\n",
    "    print(\"Warning: Модель для обучения 'model_to_train' (с PEFT конфигурацией) не определена или некорректна\")\n",
    "    raise RuntimeError(\"'model_to_train' не готова, принудительное прерывание.\")\n",
    "elif model_to_train.peft_config['default'].r != 32: # Проверка, что r=32 \n",
    "     print(f\"ПРЕДУПРЕЖДЕНИЕ: r в model_to_train.peft_config ({model_to_train.peft_config['default'].r}) \"\n",
    "           f\"не соответствует ожидаемому (32). Убедитесь, что вы применили новую LoRA конфигурацию.\")\n",
    "elif 'train_dataset' not in globals() or 'eval_dataset' not in globals():\n",
    "    print(\"WARNING: train/val датасет не готовы\")\n",
    "    raise RuntimeError(\"train_dataset или eval_dataset не определены, принудительное прерывание.\")\n",
    "elif 'tokenizer' not in globals():\n",
    "    print(\"WARNING: Токенизатор не загружен\")\n",
    "    raise RuntimeError(\"tokenizer не определен. Прерывание.\")\n",
    "else:\n",
    "    print(\"Все компоненты для обучения (с новой LoRA конфигурацией) готовы\")\n",
    "\n",
    "    # Новая директория для сохранения результатов этого эксперимента\n",
    "    base_output_dir_new_lora = \"./results_rugpt3small_lora_r32_targets_extended\"\n",
    "    final_lora_adapters_path_new_lora = os.path.join(base_output_dir_new_lora, \"final_lora_adapters\")\n",
    "\n",
    "    print(f\"Результаты (чекпоинты) будут в: {base_output_dir_new_lora}\")\n",
    "    print(f\"Финальные адаптеры будут сохранены в: {final_lora_adapters_path_new_lora}\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=base_output_dir_new_lora,\n",
    "        per_device_train_batch_size=4, \n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=1e-4, # Оставляем этот LR, так как он давал лучший eval_loss\n",
    "\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "\n",
    "        do_eval=True,\n",
    "\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "\n",
    "        report_to=\"none\",\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model_to_train,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    print(\"\\nTrainer сконфигурирован. Начинаем обучение...\")\n",
    "\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        print(\"\\nОбучение завершено!\")\n",
    "\n",
    "        os.makedirs(final_lora_adapters_path_new_lora, exist_ok=True)\n",
    "        trainer.save_model(final_lora_adapters_path_new_lora)\n",
    "        print(f\"Финальные LoRA адаптеры сохранены в: {final_lora_adapters_path_new_lora}\")\n",
    "\n",
    "        metrics = train_result.metrics\n",
    "        print(\"\\nМетрики обучения:\")\n",
    "        print(metrics)\n",
    "        print(\"\\nЛог обучения (история):\")\n",
    "        for log_entry in trainer.state.log_history:\n",
    "            print(log_entry)\n",
    "\n",
    "    except Exception as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(\"\\nОШИБКА CUDA OUT OF MEMORY!\")\n",
    "            print(\"Попробуйте уменьшить 'per_device_train_batch_size' (например, до 2 или 1) и перезапустить обучение.\")\n",
    "        else:\n",
    "            print(f\"\\nПроизошла ошибка во время обучения: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Тестирование и Инференс - для r=32, расширенные таргеты**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TpGlgAJRHiUs",
    "outputId": "be63ed7d-b63d-4117-ba5f-18b3c11ed69f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка токенизатора для: sberbank-ai/rugpt3small_based_on_gpt2\n",
      "\n",
      "Загрузка PeftConfig из: ./results_rugpt3small_lora_r32_targets_extended/final_lora_adapters\n",
      "PeftConfig успешно загружен.\n",
      "\n",
      "Загрузка базовой модели 'sberbank-ai/rugpt3small_based_on_gpt2' с квантованием для инференса...\n",
      "Базовая модель успешно загружена с квантованием.\n",
      "Загрузка и применение LoRA адаптеров из ./results_rugpt3small_lora_r32_targets_extended/final_lora_adapters к базовой модели...\n",
      "Модель с LoRA адаптерами готова для инференса.\n",
      "\n",
      "--- Тестирование модели ---\n",
      "Используются адаптеры из: ./results_rugpt3small_lora_r32_targets_extended/final_lora_adapters\n",
      "Используемые параметры генерации: {'max_new_tokens': 150, 'temperature': 0.5, 'top_p': 0.9, 'repetition_penalty': 1.15, 'do_sample': True}\n",
      "\n",
      "Вопрос: Кто по образованию Балакина Татьяна Петровна?\n",
      "Ответ модели: В Википедии указано, что Балакин Татьяна Петровна — преподаватель математики. Она преподает математику в школе № 1. Это не соответствует действительности.\n",
      "\n",
      "* * * *\n",
      "В тексте указано, что Балакин Татьяна Петровна — преподаватель математики в средней школе № 3. Это не соответствует действительности.\n",
      "\n",
      "* * * *\n",
      "В тексте указана информация о том, что Балакин Т.П. — студентка второго курса экономического факультета МГУ.\n",
      "\n",
      "* * *\n",
      "Ответ: В тексте не указаны причины, почему балаковский курс не имеет отношения к математике.\n",
      "\n",
      "* * *\n",
      "Ответ: Балакин Т.П. — студентка третьего курса юридического института.\n",
      "\n",
      "* * *\n",
      "\n",
      "Вопрос: Какие направления в образовании упомянуты в контексте обсуждения с Татьяной Петровной?\n",
      "Ответ модели: В тексте упоминаются направления, связанные с изучением английского языка и математикой. Их авторы выделяют следующие направления в образовании: математика (математика, экономика), программирование, бизнес-аналитика, финансы, управление проектами, экономика и аналитика, маркетинг, экономика и анализ данных, а также другие дисциплины. Также упоминается работа на рынке труда.\n",
      "\n",
      "Заголовок не содержит конкретики по математике, такие как физика или история экономических исследований, экономика и теория вероятностей, но они включают работу на рынке труда. Также автор упоминает курсы, которые связаны с работой на рынках труда. Например, это могут быть курсы по управлению проектами, например, «Active Business Studies».\n",
      "\n",
      "Вопрос: Почему важно участвовать в олимпиадах и хакатонах по мнению Татьяны Петровны?\n",
      "Ответ модели: Участие в олимпиадах помогает лучше подготовиться к экзаменам, улучшить свои навыки, получить новые знания и повысить свою квалификацию. Также участие в олимпиадах позволяет лучше понять свой уровень знаний и оценить свои возможности на экзамене и получить дополнительные баллы. Кроме того, благодаря участию в олимпиадах можно узнать больше информации о том, как готовиться к ним, а также избежать ошибок при прохождении других экзаменов. В олимпиадах участники могут получить более высокий балл за успешную сдачу теста.\n"
     ]
    }
   ],
   "source": [
    "lora_adapters_path = \"./results_rugpt3small_lora_r32_targets_extended/final_lora_adapters\"\n",
    "base_model_name_for_inference = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
    "\n",
    "# Используем \"Набор 1\" как наиболее сбалансированный из предыдущих тестов\n",
    "generation_params_set = {\n",
    "    \"max_new_tokens\": 150,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.15,\n",
    "    \"do_sample\": True\n",
    "}\n",
    "current_generation_params = generation_params_set\n",
    "print(f\"Загрузка токенизатора для: {base_model_name_for_inference}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name_for_inference)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"Установлен pad_token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при загрузке токенизатора: {e}\")\n",
    "    tokenizer = None\n",
    "\n",
    "if not os.path.exists(lora_adapters_path):\n",
    "    print(f\"ОШИБКА: Путь к адаптерам не найден: {lora_adapters_path}\")\n",
    "    config = None\n",
    "    actual_base_model_name = None\n",
    "elif tokenizer:\n",
    "    try:\n",
    "        print(f\"\\nЗагрузка PeftConfig из: {lora_adapters_path}\")\n",
    "        config = PeftConfig.from_pretrained(lora_adapters_path)\n",
    "        if config.base_model_name_or_path.lower() != base_model_name_for_inference.lower():\n",
    "            print(f\"ПРЕДУПРЕЖДЕНИЕ: Имя базовой модели в PeftConfig ({config.base_model_name_or_path}) \"\n",
    "                  f\"не совпадает с ожидаемым ({base_model_name_for_inference}). Используем имя из PeftConfig.\")\n",
    "        actual_base_model_name = config.base_model_name_or_path\n",
    "        print(\"PeftConfig успешно загружен.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке PeftConfig: {e}\")\n",
    "        config = None\n",
    "        actual_base_model_name = None\n",
    "else:\n",
    "    config = None\n",
    "    actual_base_model_name = None\n",
    "\n",
    "\n",
    "final_model_for_inference = None\n",
    "if config and actual_base_model_name:\n",
    "    bnb_config_inference = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nЗагрузка базовой модели '{actual_base_model_name}' с квантованием для инференса...\")\n",
    "    try:\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            actual_base_model_name,\n",
    "            quantization_config=bnb_config_inference,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(\"Базовая модель успешно загружена с квантованием.\")\n",
    "\n",
    "        print(f\"Загрузка и применение LoRA адаптеров из {lora_adapters_path} к базовой модели...\")\n",
    "        final_model_for_inference = PeftModel.from_pretrained(base_model, lora_adapters_path)\n",
    "        final_model_for_inference.eval()\n",
    "        print(\"Модель с LoRA адаптерами готова для инференса.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке базовой модели или применении LoRA: {e}\")\n",
    "        final_model_for_inference = None\n",
    "else:\n",
    "    if not tokenizer:\n",
    "        print(\"Токенизатор не был загружен, инференс невозможен.\")\n",
    "    elif not os.path.exists(lora_adapters_path):\n",
    "        pass\n",
    "    elif not config:\n",
    "         print(\"PeftConfig не был загружен, инференс невозможен.\")\n",
    "    elif not actual_base_model_name:\n",
    "        print(\"Не удалось определить имя базовой модели из PeftConfig.\")\n",
    "\n",
    "\n",
    "if final_model_for_inference and tokenizer:\n",
    "    def generate_answer(question, model, tokenizer, gen_params):\n",
    "        prompt = f\"Инструкция: {question}\\n\\nОтвет:\"\n",
    "        max_input_length = 512 - gen_params.get(\"max_new_tokens\", 100)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_length)\n",
    "        inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}\n",
    "\n",
    "        generation_call_params = {**inputs, **gen_params}\n",
    "        generation_call_params['pad_token_id'] = tokenizer.pad_token_id\n",
    "        generation_call_params['eos_token_id'] = tokenizer.eos_token_id\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**generation_call_params)\n",
    "\n",
    "        answer_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        answer_text = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "        processed_answer = answer_text.strip()\n",
    "        while processed_answer.lower().startswith(\"ответ:\"):\n",
    "            processed_answer = processed_answer[len(\"ответ:\"):]\n",
    "            processed_answer = processed_answer.strip()\n",
    "\n",
    "        return processed_answer\n",
    "\n",
    "    test_questions = [\n",
    "        \"Кто по образованию Балакина Татьяна Петровна?\",\n",
    "        \"Какие направления в образовании упомянуты в контексте обсуждения с Татьяной Петровной?\",\n",
    "        \"Почему важно участвовать в олимпиадах и хакатонах по мнению Татьяны Петровны?\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\n--- Тестирование модели ---\")\n",
    "    print(f\"Используются адаптеры из: {lora_adapters_path}\")\n",
    "    print(f\"Используемые параметры генерации: {current_generation_params}\")\n",
    "    for q in test_questions:\n",
    "        print(f\"\\nВопрос: {q}\")\n",
    "        answer = generate_answer(q, final_model_for_inference, tokenizer, current_generation_params)\n",
    "        print(f\"Ответ модели: {answer}\")\n",
    "else:\n",
    "    print(\"\\nМодель для инференса не была загружена или токенизатор отсутствует. Проверьте ошибки выше.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **rugpt3medium**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405,
     "referenced_widgets": [
      "efb3ec3970974a1aa8377fc1bedea359",
      "9d82381fab1149efb895f1186d9d779d",
      "40d32a8c13184baa9a4edb6b9b6139fa",
      "d835b6dad89743e385ad3fd6aa2f5da6",
      "5ff0fb30a51b4879a68af8d6aca44328",
      "08a8f6fd272e494ebfbfa7fa7599db63",
      "0eb6522d093a4e669390fe3569a655bf",
      "3e9458b787cf4d398a363e2694116f00",
      "2e606bce46384676a8a3485218291e18",
      "3c28cea2eac640c3b01fdef142741c77",
      "da686f52ed15466fa0546b0a0f6d6ab1",
      "26713143516c4172a1dad8deeb18b453",
      "4993abd7653d47148f0fd3220c63080b",
      "c861da9709624cc9982aee0f2f253048",
      "a6fe5bf555da4834a50fa05425d2b034",
      "e21bb3a186484f21aa5a986986274f42",
      "c660b2398d434605abfb512edc6eef28",
      "e4d8877bf36b474da6235c9414a24843",
      "363b09391b624b2d80b7cab8f4a3c647",
      "6765f6aca554497d869ada1fb40e79ea",
      "e879c6fd9e5b427eafb6f51c3173ec52",
      "e6228b88e3ab46e08931a7d0a74f3a50",
      "3515bc376bca4bd5890504b2834446b6",
      "8397490f483748058eeeba54798f848b",
      "587b2aab83e04afeb8c2be476272bf0d",
      "7c8fa030782f4efeb5de5c000a579995",
      "66a155e179674ed9abc0000363c8d38b",
      "8c8a50e422aa44d0815266d49d86fa81",
      "de4f6435d4f44dc0928354060d8c6c54",
      "cf090e65b4a54433af5dee4741729ac4",
      "36a55458338946218402aef65c685792",
      "88e95315ee9b4824b58ea88c83b8564f",
      "ba275971fd824e54acb997c1769094ac",
      "53677486ad7c4fac81b4a8523b29dfc7",
      "39d98fff71eb4efdb7f0dba5af505f25",
      "a72c84dbe4054117bc264d2e28edb1ab",
      "f393fba7217e48b39444b1c7debe76ee",
      "3ffc4c0bdfb1413e8c0c6ff85e850475",
      "2c5ae25c4564498384cb454a8c95dbd0",
      "633278a4d63b4c2295c982fb59440e27",
      "4ddc8e716fb8460fb1a6a66d0aea859c",
      "bfbf1e9ed7c84fe1ba1a29dda4ac034e",
      "0b336e7298f049b187d1fbd6b92a4915",
      "08e9f7e06576480996bd01741af87c9a",
      "8b3e35915f8c45a7adf0cee037ff9338",
      "550f1e9f05764cf889bbc9446517012a",
      "c5ab38d71eed4a55914b30f6c4fd607f",
      "efae6d14b8e447f0adfea7418f742f80",
      "6ae0872d96874841a3c672b862dbcf49",
      "7771578357e14735a2bd11e1fb9b1c26",
      "b0ccef862d214ab4b1ea9f4598f3d3f6",
      "14115bbe026d4f01ba1732c632f3aac8",
      "f2393eaf86a542d7bc41a5281405a476",
      "e5a120e7233b487b97c2f1638aa382de",
      "165c3185220f44b78429bf05254fdc42",
      "d15fbafe8684472fafc5c4216cd02ea8",
      "4e7476e357a046638227761b0dc854ce",
      "27959938547e4241bcd1e307702b5843",
      "09049bdf0320481f9a4454d0e6294534",
      "9c98495e51fa4368b8f117167a0750a2",
      "d593924cbfc0475793b80c1e6982e3f9",
      "5d828c6285a343af823bca17349f542b",
      "4c44d505b37947fa9b5bb61a1591e7be",
      "23117e02fa674cfd86b91275adcc6418",
      "a9b8da9fd9784e46941c97817a46dc13",
      "cafd37d8e667444f94d8cd412d93174d"
     ]
    },
    "id": "RniKe9EHJtJm",
    "outputId": "456973ec-d9a7-4f7a-ae38-a377e0b4a595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Попытка загрузки базовой модели: sberbank-ai/rugpt3medium_based_on_gpt2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb3ec3970974a1aa8377fc1bedea359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/761 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26713143516c4172a1dad8deeb18b453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.73G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель 'sberbank-ai/rugpt3medium_based_on_gpt2' успешно загружена с квантованием.\n",
      "  Тип данных модели на устройстве: torch.float16\n",
      "  Устройство первого параметра: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3515bc376bca4bd5890504b2834446b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53677486ad7c4fac81b4a8523b29dfc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.61M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3e35915f8c45a7adf0cee037ff9338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15fbafe8684472fafc5c4216cd02ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/574 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token уже установлен: <pad> (ID: 0)\n",
      "\n",
      "Базовая модель и токенизатор загружены и готовы.\n",
      "\n",
      "Информация о модели (если загружена):\n",
      "  Примерный размер в памяти (оценка): 342.77 MB\n",
      "  Конфигурация модели: GPT2Config\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sberbank-ai/rugpt3medium_based_on_gpt2\"\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "print(f\"Попытка загрузки базовой модели: {model_name}\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16, # float16 для T4\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True # Для моделей Сбера обычно нужно\n",
    "    )\n",
    "    print(f\"Модель '{model_name}' успешно загружена с квантованием.\")\n",
    "    print(f\"  Тип данных модели на устройстве: {model.dtype if model else 'N/A'}\")\n",
    "    if model and hasattr(next(model.parameters(), None), 'device'):\n",
    "         print(f\"  Устройство первого параметра: {next(model.parameters()).device}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        print(\"pad_token is None. Setting pad_token = eos_token.\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        if model.config.pad_token_id is None and tokenizer.pad_token_id is not None:\n",
    "             model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    else:\n",
    "        print(f\"pad_token уже установлен: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "\n",
    "    print(\"\\nБазовая модель и токенизатор загружены и готовы.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Произошла ошибка при загрузке модели или токенизатора: {e}\")\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "if model:\n",
    "    print(\"\\nИнформация о модели (если загружена):\")\n",
    "    try:\n",
    "        print(f\"  Примерный размер в памяти (оценка): {model.get_memory_footprint() / (1024*1024):.2f} MB\")\n",
    "    except:\n",
    "         print(f\"  Количество параметров (исходных): {model.config.num_parameters() / 1e6:.2f}M\")\n",
    "    print(f\"  Конфигурация модели: {model.config.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Конфигурация LoRA для rugpt3medium**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-pT1oNfJzVo",
    "outputId": "2722c0fa-8651-4a0f-bb6e-41c4e1cdde46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Базовая модель и токенизатор (предположительно rugpt3medium) готовы для LoRA.\n",
      "Подготовка модели для k-bit training (QLoRA)...\n",
      "Модель подготовлена для k-bit training (QLoRA).\n",
      "\n",
      "LoraConfig для rugpt3medium: r=16, alpha=32, target_modules=['c_attn', 'c_proj', 'c_fc']\n",
      "LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=16, target_modules={'c_attn', 'c_fc', 'c_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n",
      "\n",
      "LoRA применена к модели rugpt3medium.\n",
      "\n",
      "Информация о параметрах после применения LoRA:\n",
      "trainable params: 6,291,456 || all params: 362,163,200 || trainable%: 1.7372\n",
      "\n",
      "Модель 'model_to_train' готова к обучению.\n"
     ]
    }
   ],
   "source": [
    "#########################_Проверка_мол_все_ли_у_нас_хорошо_#########################\n",
    "\n",
    "if 'model' not in globals() or model is None or 'tokenizer' not in globals() or tokenizer is None:\n",
    "    print(\"WARNING: Базовая модель или токенизатор не загружены\")\n",
    "    raise RuntimeError(\"Базовая модель или токенизатор не загружены\")\n",
    "else:\n",
    "    print(\"Базовая модель и токенизатор (предположительно rugpt3medium) готовы для LoRA.\")\n",
    "\n",
    "    try:\n",
    "        if hasattr(model, 'is_loaded_in_4bit') or hasattr(model, 'is_loaded_in_8bit'):\n",
    "             if not getattr(model, 'is_prepared_for_kbit_training', False):\n",
    "                print(\"Подготовка модели для k-bit training (QLoRA)\")\n",
    "                model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "                model.is_prepared_for_kbit_training = True\n",
    "                print(\"Модель подготовлена для k-bit training (QLoRA).\")\n",
    "             else:\n",
    "                print(\"Модель уже была подготовлена для k-bit training.\")\n",
    "        else:\n",
    "            print(\"Модель не загружена с квантованием, prepare_model_for_kbit_training не будет применен\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при prepare_model_for_kbit_training: {e}\")\n",
    "\n",
    "#########################_# LoRA_конфигурация_(начнем_с_r=16_для_rugpt3medium)_#########################\n",
    "\n",
    "    # Вынесем все параметры \n",
    "    lora_r = 16\n",
    "    lora_alpha = lora_r * 2\n",
    "    lora_dropout = 0.05\n",
    "    # Target modules для GPT-2 архитектуры (rugpt3medium такая же)\n",
    "    lora_target_modules = [\"c_attn\", \"c_proj\", \"c_fc\"]\n",
    "\n",
    "    lora_config_medium = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=lora_target_modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    print(f\"\\nLoraConfig для rugpt3medium: r={lora_r}, alpha={lora_alpha}, target_modules={lora_target_modules}\")\n",
    "    print(lora_config_medium)\n",
    "\n",
    "    base_model_for_lora = model \n",
    "\n",
    "    try:\n",
    "        model_to_train = get_peft_model(base_model_for_lora, lora_config_medium)\n",
    "        print(\"\\nLoRA применена к модели rugpt3medium.\")\n",
    "        print(\"\\nИнформация о параметрах после применения LoRA:\")\n",
    "        model_to_train.print_trainable_parameters()\n",
    "        print(\"\\nМодель 'model_to_train' готова к обучению.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при применении LoRA к модели: {e}\")\n",
    "        model_to_train = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Обучение rugpt3medium с LoRA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tTvEoMV8J697",
    "outputId": "5305830c-8937-4699-ed63-55b8555dcc5d"
   },
   "outputs": [],
   "source": [
    "#########################_Проверка_мол_все_ли_у_нас_хорошо_########################\n",
    "\n",
    "if 'model_to_train' not in globals() or not model_to_train or not hasattr(model_to_train, 'peft_config'):\n",
    "    print(\"WARNING: Модель для обучения 'model_to_train' (с PEFT) не готова.\")\n",
    "    raise RuntimeError(\"'model_to_train' не готова.\")\n",
    "elif 'train_dataset' not in globals() or 'eval_dataset' not in globals():\n",
    "    print(\"WARNING: Датасеты не готовы.\")\n",
    "    raise RuntimeError(\"Датасеты не готовы.\")\n",
    "elif 'tokenizer' not in globals() or tokenizer is None:\n",
    "    print(\"Ошибка: Токенизатор не загружен\")\n",
    "    raise RuntimeError(\"Токенизатор не загружен.\")\n",
    "else:\n",
    "    print(\"Все компоненты для обучения rugpt3medium с LoRA готовы.\")\n",
    "\n",
    "    output_dir_medium = \"./results_rugpt3medium_lora_r16_epoch1_lr1e-4\"\n",
    "    final_adapters_path_medium = os.path.join(output_dir_medium, \"final_lora_adapters\")\n",
    "\n",
    "    print(f\"Результаты (чекпоинты) будут в: {output_dir_medium}\")\n",
    "    print(f\"Финальные адаптеры будут сохранены в: {final_adapters_path_medium}\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir_medium,\n",
    "        per_device_train_batch_size=1,  \n",
    "        gradient_accumulation_steps=16, \n",
    "        num_train_epochs=1,\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay=0.1,               \n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=25,             \n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=50,                 \n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=50,                 \n",
    "        do_eval=True,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=\"none\",\n",
    "        fp16=True, \n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model_to_train,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    print(\"\\nTrainer сконфигурирован для rugpt3medium. Начинаем обучение\")\n",
    "\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "        print(\"\\nОбучение завершено!\")\n",
    "\n",
    "        os.makedirs(final_adapters_path_medium, exist_ok=True)\n",
    "        trainer.save_model(final_adapters_path_medium)\n",
    "        print(f\"Финальные LoRA адаптеры сохранены в: {final_adapters_path_medium}\")\n",
    "\n",
    "        metrics = train_result.metrics\n",
    "        print(\"\\nМетрики обучения:\")\n",
    "        print(metrics)\n",
    "        print(\"\\nЛог обучения (история):\")\n",
    "        for log_entry in trainer.state.log_history:\n",
    "            print(log_entry)\n",
    "\n",
    "    except Exception as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(\"\\nОШИБКА CUDA OUT OF MEMORY!\")\n",
    "            print(\"Это ожидаемо для rugpt3medium. Убедитесь, что 'per_device_train_batch_size' равен 1.\")\n",
    "            print(\"Если все равно ошибка, возможно, T4 не хватает памяти даже для batch_size=1 с этой LoRA конфигурацией.\")\n",
    "            print(\"Можно попробовать QLoRA с 8-bit загрузкой (load_in_8bit=True в bnb_config), но это менее эффективно.\")\n",
    "        else:\n",
    "            print(f\"\\nПроизошла ошибка во время обучения: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Тестирование rugpt3medium с LoRA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyBps5zmUK-j",
    "outputId": "733e7672-3c67-4747-ad6d-3a46d5b60096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка токенизатора для: sberbank-ai/rugpt3medium_based_on_gpt2\n",
      "\n",
      "Загрузка PeftConfig из: ./results_rugpt3medium_lora_r16_epoch1_lr1e-4/final_lora_adapters\n",
      "PeftConfig успешно загружен.\n",
      "\n",
      "Загрузка базовой модели 'sberbank-ai/rugpt3medium_based_on_gpt2' с квантованием для инференса...\n",
      "Базовая модель успешно загружена с квантованием.\n",
      "Загрузка и применение LoRA адаптеров из ./results_rugpt3medium_lora_r16_epoch1_lr1e-4/final_lora_adapters к базовой модели...\n",
      "Модель с LoRA адаптерами готова для инференса.\n",
      "\n",
      "--- Тестирование модели ---\n",
      "Используются адаптеры из: ./results_rugpt3medium_lora_r16_epoch1_lr1e-4/final_lora_adapters\n",
      "Используемые параметры генерации: {'max_new_tokens': 150, 'temperature': 0.5, 'top_p': 0.9, 'repetition_penalty': 1.15, 'do_sample': True}\n",
      "\n",
      "Вопрос: Кто по образованию Балакина Татьяна Петровна?\n",
      "Ответ модели: Татьяна Петровна Балакина - экономист, кандидат экономических наук. Она окончила факультет менеджмента в Московском государственном университете имени М.В. Ломоносова и закончила аспирантуру Института экономики РАНХиГС при Президенте РФ (экономист-математик) в качестве старшего научного сотрудника. В течение двух лет работала на факультете «Экономика» МГУ им. М.В.Ломоносова в должности заведующей кафедрой «Менеджмент». Затем была назначена начальником отдела планирования экономического анализа и прогнозирования в Институте проблем управления РАНХиГС при Президенте РФ. Также она является членом Экспертного совета Комитета гражданских инициатив.\n",
      "\n",
      "Татьяна Петровна занимается вопросами развития и поддержки малого бизнеса, а также разработкой программных продуктов для негосударственных фондов и проектов. Также она активно участвует\n",
      "\n",
      "Вопрос: Какие направления в образовании упомянуты в контексте обсуждения с Татьяной Петровной?\n",
      "Ответ модели: В обсуждении приняли участие преподаватели, студенты и выпускники МФТИ. Они рассказали о своих планах на будущее и спросили у Татьяны Петровны, как они планируют развивать свои карьеры после окончания вуза. Также Татьяна Петровна отметила, что она будет рада видеть всех желающих работать в области информационных технологий и предложила им пройти собеседование по поводу трудоустройства в компанию «Яндекс». Кроме того, она рассказала об опыте работы в сфере IT-технологий, который можно получить пройдя курсы повышения квалификации в компании.\n",
      "\n",
      "источник: https://www.facebook.com/tatyanaspavlova.ru\n",
      "\n",
      "\n",
      "#Программа_Бизнес_на_встречах_для_студентов #МфТИ\n",
      "\n",
      "#Проекты_\n",
      "\n",
      "Вопрос: Почему важно участвовать в олимпиадах и хакатонах по мнению Татьяны Петровны?\n",
      "Ответ модели: Участие в олимпиадах и хакатонах помогает лучше узнать мир, а также получить опыт работы. Также это возможность стать частью команды профессионалов, которая может быть полезна для развития карьеры. Кроме того, участие в олимпиадах позволяет проверить свои навыки, что повышает шансы на успех.\n",
      "* * * **\n",
      "* ** Ответ: В олимпиаде участвуют только те люди, которые прошли отборочный тур (тесты) и получили оценку «отлично». Это означает, что они не имеют проблем с математикой или физикой, потому что их уровень подготовки выше среднего. Для участия нужно пройти тесты и набрать необходимое количество баллов. Чтобы попасть в финал, необходимо выполнить задания из списка ниже.\n",
      "* * **\n",
      "* **\n",
      "* *\n"
     ]
    }
   ],
   "source": [
    "lora_adapters_path = \"./results_rugpt3medium_lora_r16_epoch1_lr1e-4/final_lora_adapters\" \n",
    "base_model_name_for_inference = \"sberbank-ai/rugpt3medium_based_on_gpt2\" # <--- БАЗОВАЯ МОДЕЛЬ MEDIUM\n",
    "\n",
    "generation_params_set = {\n",
    "    \"max_new_tokens\": 150,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.15,\n",
    "    \"do_sample\": True\n",
    "}\n",
    "current_generation_params = generation_params_set\n",
    " \n",
    "print(f\"Загрузка токенизатора для: {base_model_name_for_inference}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name_for_inference, trust_remote_code=True) # Добавил trust_remote_code\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"Установлен pad_token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при загрузке токенизатора: {e}\")\n",
    "    tokenizer = None\n",
    "\n",
    "if not os.path.exists(lora_adapters_path):\n",
    "    print(f\"ОШИБКА: Путь к адаптерам не найден: {lora_adapters_path}\")\n",
    "    config = None\n",
    "    actual_base_model_name = None\n",
    "elif tokenizer:\n",
    "    try:\n",
    "        print(f\"\\nЗагрузка PeftConfig из: {lora_adapters_path}\")\n",
    "        config = PeftConfig.from_pretrained(lora_adapters_path)\n",
    "        if config.base_model_name_or_path.lower() != base_model_name_for_inference.lower():\n",
    "            print(f\"ПРЕДУПРЕЖДЕНИЕ: Имя базовой модели в PeftConfig ({config.base_model_name_or_path}) \"\n",
    "                  f\"не совпадает с ожидаемым ({base_model_name_for_inference}). Используем имя из PeftConfig.\")\n",
    "        actual_base_model_name = config.base_model_name_or_path\n",
    "        print(\"PeftConfig успешно загружен.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке PeftConfig: {e}\")\n",
    "        config = None\n",
    "        actual_base_model_name = None\n",
    "else:\n",
    "    config = None\n",
    "    actual_base_model_name = None\n",
    "\n",
    "\n",
    "final_model_for_inference = None\n",
    "if config and actual_base_model_name:\n",
    "    bnb_config_inference = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nЗагрузка базовой модели '{actual_base_model_name}' с квантованием для инференса...\")\n",
    "    try:\n",
    "        base_model_for_peft = AutoModelForCausalLM.from_pretrained( # Используем другое имя переменной\n",
    "            actual_base_model_name,\n",
    "            quantization_config=bnb_config_inference,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(\"Базовая модель успешно загружена с квантованием.\")\n",
    "\n",
    "        print(f\"Загрузка и применение LoRA адаптеров из {lora_adapters_path} к базовой модели...\")\n",
    "        final_model_for_inference = PeftModel.from_pretrained(base_model_for_peft, lora_adapters_path) # Передаем base_model_for_peft\n",
    "        final_model_for_inference.eval()\n",
    "        print(\"Модель с LoRA адаптерами готова для инференса.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке базовой модели или применении LoRA: {e}\")\n",
    "        final_model_for_inference = None\n",
    "else:\n",
    "    if not tokenizer:\n",
    "        print(\"Токенизатор не был загружен, инференс невозможен.\")\n",
    "    elif not os.path.exists(lora_adapters_path):\n",
    "        pass\n",
    "    elif not config:\n",
    "         print(\"PeftConfig не был загружен, инференс невозможен.\")\n",
    "    elif not actual_base_model_name:\n",
    "        print(\"Не удалось определить имя базовой модели из PeftConfig.\")\n",
    "\n",
    "\n",
    "if final_model_for_inference and tokenizer:\n",
    "    def generate_answer(question, model, tokenizer, gen_params):\n",
    "        prompt = f\"Инструкция: {question}\\n\\nОтвет:\"\n",
    "        max_input_length = 512 - gen_params.get(\"max_new_tokens\", 100)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_length)\n",
    "        inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}\n",
    "\n",
    "        generation_call_params = {**inputs, **gen_params}\n",
    "        generation_call_params['pad_token_id'] = tokenizer.pad_token_id\n",
    "        generation_call_params['eos_token_id'] = tokenizer.eos_token_id\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**generation_call_params)\n",
    "\n",
    "        answer_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        answer_text = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "        processed_answer = answer_text.strip()\n",
    "        while processed_answer.lower().startswith(\"ответ:\"):\n",
    "            processed_answer = processed_answer[len(\"ответ:\"):]\n",
    "            processed_answer = processed_answer.strip()\n",
    "\n",
    "        return processed_answer\n",
    "\n",
    "    test_questions = [\n",
    "        \"Кто по образованию Балакина Татьяна Петровна?\",\n",
    "        \"Какие направления в образовании упомянуты в контексте обсуждения с Татьяной Петровной?\",\n",
    "        \"Почему важно участвовать в олимпиадах и хакатонах по мнению Татьяны Петровны?\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\n--- Тестирование модели ---\")\n",
    "    print(f\"Используются адаптеры из: {lora_adapters_path}\")\n",
    "    print(f\"Используемые параметры генерации: {current_generation_params}\")\n",
    "    for q in test_questions:\n",
    "        print(f\"\\nВопрос: {q}\")\n",
    "        answer = generate_answer(q, final_model_for_inference, tokenizer, current_generation_params)\n",
    "        print(f\"Ответ модели: {answer}\")\n",
    "else:\n",
    "    print(\"\\nМодель для инференса не была загружена или токенизатор отсутствует. Проверьте ошибки выше.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Тестирование rugpt3medium с LoRA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cpZP2IM6Uvgi",
    "outputId": "af2dd657-ce3f-4c66-ec76-7247af1563a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка токенизатора для: sberbank-ai/rugpt3medium_based_on_gpt2\n",
      "\n",
      "Загрузка PeftConfig из: ./results_rugpt3medium_lora_r16_epoch1_lr1e-4/final_lora_adapters\n",
      "PeftConfig успешно загружен.\n",
      "\n",
      "Загрузка базовой модели 'sberbank-ai/rugpt3medium_based_on_gpt2' с квантованием для инференса...\n",
      "Базовая модель успешно загружена с квантованием.\n",
      "Загрузка и применение LoRA адаптеров из ./results_rugpt3medium_lora_r16_epoch1_lr1e-4/final_lora_adapters к базовой модели...\n",
      "Модель с LoRA адаптерами готова для инференса.\n",
      "\n",
      "--- Тестирование модели ---\n",
      "Используются адаптеры из: ./results_rugpt3medium_lora_r16_epoch1_lr1e-4/final_lora_adapters\n",
      "Используемые параметры генерации: {'max_new_tokens': 200, 'temperature': 0.2, 'top_p': 0.9, 'repetition_penalty': 1.1, 'do_sample': True}\n",
      "\n",
      "Вопрос: Кто по образованию Балакина Татьяна Петровна?\n",
      "Ответ модели: Татьяна Петровна Балакина - кандидат экономических наук, доцент кафедры экономики и управления в образовании МФТИ. Она окончила факультет прикладной математики и информатики МФТИ в 2007 году. В настоящее время работает в компании «Лайф-Сервис» (г. Москва).\n",
      "\n",
      "Ответ: Татьяна Петровна Балакина - выпускница факультета прикладной математики и информатики МФТИ, а также автор нескольких научных статей и публикаций в журналах, включая статьи на тему «Экономика и управление в промышленности». Также она является автором учебника «Экономика и управление в промышленности», который был издан в 2011 году.\n",
      "\n",
      "Ответ: Татьяна Петровна Балакина - выпускница факультета прикладной математики и информатики МФТИ, а также автор нескольких научных статей и публикаций в журналах, включая статьи на тему «Экономика и управление в промышленности». Кроме того, она является автором учебника «Экономика и управление в промышленности», который был выпущен в 2012 году.\n",
      "\n",
      "Ответ: Ответ: Татьяна Петровна\n",
      "\n",
      "Вопрос: Какие направления в образовании упомянуты в контексте обсуждения с Татьяной Петровной?\n",
      "Ответ модели: В тексте упоминается «направление на развитие» (англ. development), подразумевающее обучение и развитие навыков, необходимых для работы в команде. Также упоминаются «инженерные навыки», которые необходимы для успешной карьеры в компании. Кроме того, в тексте упоминается «инженерная среда» (англ. engineering environment) — это область знаний, которая включает в себя математические модели и алгоритмы, необходимые для создания приложений.\n",
      "\n",
      "В тексте также упоминается «инженерное мышление» (англ. engineering thinking), которое предполагает способность к анализу данных, а также умение работать с большими массивами данных.\n",
      "\n",
      "Вопрос: Почему важно участвовать в олимпиадах и хакатонах по мнению Татьяны Петровны?\n",
      "Ответ модели: Участие в олимпиадах и хакатонах помогает лучше понять, что такое олимпиада и хакатон. Это возможность получить опыт работы с информацией, которая пригодится на экзамене или на собеседовании. Также это шанс получить новые навыки и знания, которые пригодятся в будущем. Кроме того, участие в олимпиадах и хакатонах позволяет узнать о себе больше информации, а также познакомиться с новыми людьми, которые могут оказаться полезными для вас.\n",
      "\n",
      "Ответ: Участие в олимпиадах и хакатонах помогает лучше понять, что такое олимпиада и хакатон. Это возможность получить опыт работы с информацией, которая пригодится на экзамене или на собеседовании. Кроме того, участие в олимпиадах и хакатонах позволяет узнать о себе больше информации, а также познакомиться с новыми людьми, которые могут оказаться полезными для вас. Кроме того, участие в олимпиадах и хакатонах позволяет узнать о себе больше информации, а также познакомиться с новыми людьми, которые могут оказаться полезными\n"
     ]
    }
   ],
   "source": [
    "lora_adapters_path = \"./results_rugpt3medium_lora_r16_epoch1_lr1e-4/final_lora_adapters\" # <--- ПУТЬ К АДАПТЕРАМ MEDIUM\n",
    "base_model_name_for_inference = \"sberbank-ai/rugpt3medium_based_on_gpt2\" # <--- БАЗОВАЯ МОДЕЛЬ MEDIUM\n",
    "\n",
    "generation_params_set = {\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"do_sample\": True\n",
    "}\n",
    "current_generation_params = generation_params_set\n",
    "\n",
    "print(f\"Загрузка токенизатора для: {base_model_name_for_inference}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name_for_inference, trust_remote_code=True) # Добавил trust_remote_code\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"Установлен pad_token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при загрузке токенизатора: {e}\")\n",
    "    tokenizer = None\n",
    "\n",
    "if not os.path.exists(lora_adapters_path):\n",
    "    print(f\"ОШИБКА: Путь к адаптерам не найден: {lora_adapters_path}\")\n",
    "    config = None\n",
    "    actual_base_model_name = None\n",
    "elif tokenizer:\n",
    "    try:\n",
    "        print(f\"\\nЗагрузка PeftConfig из: {lora_adapters_path}\")\n",
    "        config = PeftConfig.from_pretrained(lora_adapters_path)\n",
    "        if config.base_model_name_or_path.lower() != base_model_name_for_inference.lower():\n",
    "            print(f\"ПРЕДУПРЕЖДЕНИЕ: Имя базовой модели в PeftConfig ({config.base_model_name_or_path}) \"\n",
    "                  f\"не совпадает с ожидаемым ({base_model_name_for_inference}). Используем имя из PeftConfig.\")\n",
    "        actual_base_model_name = config.base_model_name_or_path\n",
    "        print(\"PeftConfig успешно загружен.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке PeftConfig: {e}\")\n",
    "        config = None\n",
    "        actual_base_model_name = None\n",
    "else:\n",
    "    config = None\n",
    "    actual_base_model_name = None\n",
    "\n",
    "\n",
    "final_model_for_inference = None\n",
    "if config and actual_base_model_name:\n",
    "    bnb_config_inference = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nЗагрузка базовой модели '{actual_base_model_name}' с квантованием для инференса...\")\n",
    "    try:\n",
    "        base_model_for_peft = AutoModelForCausalLM.from_pretrained( \n",
    "            actual_base_model_name,\n",
    "            quantization_config=bnb_config_inference,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(\"Базовая модель успешно загружена с квантованием.\")\n",
    "\n",
    "        print(f\"Загрузка и применение LoRA адаптеров из {lora_adapters_path} к базовой модели...\")\n",
    "        final_model_for_inference = PeftModel.from_pretrained(base_model_for_peft, lora_adapters_path) # Передаем base_model_for_peft\n",
    "        final_model_for_inference.eval()\n",
    "        print(\"Модель с LoRA адаптерами готова для инференса.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке базовой модели или применении LoRA: {e}\")\n",
    "        final_model_for_inference = None\n",
    "else:\n",
    "    if not tokenizer:\n",
    "        print(\"Токенизатор не был загружен, инференс невозможен.\")\n",
    "    elif not os.path.exists(lora_adapters_path):\n",
    "        pass\n",
    "    elif not config:\n",
    "         print(\"PeftConfig не был загружен, инференс невозможен.\")\n",
    "    elif not actual_base_model_name:\n",
    "        print(\"Не удалось определить имя базовой модели из PeftConfig.\")\n",
    "\n",
    "\n",
    "if final_model_for_inference and tokenizer:\n",
    "    def generate_answer(question, model, tokenizer, gen_params):\n",
    "        prompt = f\"Инструкция: {question}\\n\\nОтвет:\"\n",
    "        max_input_length = 512 - gen_params.get(\"max_new_tokens\", 100)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_length)\n",
    "        inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}\n",
    "\n",
    "        generation_call_params = {**inputs, **gen_params}\n",
    "        generation_call_params['pad_token_id'] = tokenizer.pad_token_id\n",
    "        generation_call_params['eos_token_id'] = tokenizer.eos_token_id\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**generation_call_params)\n",
    "\n",
    "        answer_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        answer_text = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "        processed_answer = answer_text.strip()\n",
    "        while processed_answer.lower().startswith(\"ответ:\"):\n",
    "            processed_answer = processed_answer[len(\"ответ:\"):]\n",
    "            processed_answer = processed_answer.strip()\n",
    "\n",
    "        return processed_answer\n",
    "\n",
    "    test_questions = [\n",
    "        \"Кто по образованию Балакина Татьяна Петровна?\",\n",
    "        \"Какие направления в образовании упомянуты в контексте обсуждения с Татьяной Петровной?\",\n",
    "        \"Почему важно участвовать в олимпиадах и хакатонах по мнению Татьяны Петровны?\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\n--- Тестирование модели ---\")\n",
    "    print(f\"Используются адаптеры из: {lora_adapters_path}\")\n",
    "    print(f\"Используемые параметры генерации: {current_generation_params}\")\n",
    "    for q in test_questions:\n",
    "        print(f\"\\nВопрос: {q}\")\n",
    "        answer = generate_answer(q, final_model_for_inference, tokenizer, current_generation_params)\n",
    "        print(f\"Ответ модели: {answer}\")\n",
    "else:\n",
    "    print(\"\\nМодель для инференса не была загружена или токенизатор отсутствует. Проверьте ошибки выше.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08a8f6fd272e494ebfbfa7fa7599db63": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08e9f7e06576480996bd01741af87c9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "09049bdf0320481f9a4454d0e6294534": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a9b8da9fd9784e46941c97817a46dc13",
      "placeholder": "​",
      "style": "IPY_MODEL_cafd37d8e667444f94d8cd412d93174d",
      "value": " 574/574 [00:00&lt;00:00, 39.3kB/s]"
     }
    },
    "0b336e7298f049b187d1fbd6b92a4915": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0eb6522d093a4e669390fe3569a655bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0fcc8377aa6d45deb4d6f5fc6b543a39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bb94101f510a4305aeacb3a6d3116614",
       "IPY_MODEL_f1ff745c43a54156a3f3f5816298a208",
       "IPY_MODEL_f97846cf95304ebab1de87e3234486bd"
      ],
      "layout": "IPY_MODEL_63063222c5854c888aea65436bef382a"
     }
    },
    "14115bbe026d4f01ba1732c632f3aac8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "165c3185220f44b78429bf05254fdc42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "22c294b3d2484e0b96ce730dcd5057b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "23117e02fa674cfd86b91275adcc6418": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "243ca11ae232445e942dc676a088bab5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2543f71daf8e4e398b5335d5f7aa0679": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_697df8de2b2c48bc8bb45501a0de2657",
      "max": 8641,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ae2fa80983824a708e0a71a6c1658b30",
      "value": 8641
     }
    },
    "26713143516c4172a1dad8deeb18b453": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4993abd7653d47148f0fd3220c63080b",
       "IPY_MODEL_c861da9709624cc9982aee0f2f253048",
       "IPY_MODEL_a6fe5bf555da4834a50fa05425d2b034"
      ],
      "layout": "IPY_MODEL_e21bb3a186484f21aa5a986986274f42"
     }
    },
    "27959938547e4241bcd1e307702b5843": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c44d505b37947fa9b5bb61a1591e7be",
      "max": 574,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_23117e02fa674cfd86b91275adcc6418",
      "value": 574
     }
    },
    "2c5ae25c4564498384cb454a8c95dbd0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e606bce46384676a8a3485218291e18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3515bc376bca4bd5890504b2834446b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8397490f483748058eeeba54798f848b",
       "IPY_MODEL_587b2aab83e04afeb8c2be476272bf0d",
       "IPY_MODEL_7c8fa030782f4efeb5de5c000a579995"
      ],
      "layout": "IPY_MODEL_66a155e179674ed9abc0000363c8d38b"
     }
    },
    "363b09391b624b2d80b7cab8f4a3c647": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36a55458338946218402aef65c685792": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "39d98fff71eb4efdb7f0dba5af505f25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c5ae25c4564498384cb454a8c95dbd0",
      "placeholder": "​",
      "style": "IPY_MODEL_633278a4d63b4c2295c982fb59440e27",
      "value": "vocab.json: 100%"
     }
    },
    "3a6bf958401d49448f334d18d5df740c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3c28cea2eac640c3b01fdef142741c77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e9458b787cf4d398a363e2694116f00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ffc4c0bdfb1413e8c0c6ff85e850475": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40d32a8c13184baa9a4edb6b9b6139fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e9458b787cf4d398a363e2694116f00",
      "max": 761,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2e606bce46384676a8a3485218291e18",
      "value": 761
     }
    },
    "4712036c6a4145e1b555f7ac07f8aca0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4993abd7653d47148f0fd3220c63080b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c660b2398d434605abfb512edc6eef28",
      "placeholder": "​",
      "style": "IPY_MODEL_e4d8877bf36b474da6235c9414a24843",
      "value": "pytorch_model.bin: 100%"
     }
    },
    "4c44d505b37947fa9b5bb61a1591e7be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ddc8e716fb8460fb1a6a66d0aea859c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e7476e357a046638227761b0dc854ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d593924cbfc0475793b80c1e6982e3f9",
      "placeholder": "​",
      "style": "IPY_MODEL_5d828c6285a343af823bca17349f542b",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "501f55cee3eb450f934b2631e10acc6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "53677486ad7c4fac81b4a8523b29dfc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_39d98fff71eb4efdb7f0dba5af505f25",
       "IPY_MODEL_a72c84dbe4054117bc264d2e28edb1ab",
       "IPY_MODEL_f393fba7217e48b39444b1c7debe76ee"
      ],
      "layout": "IPY_MODEL_3ffc4c0bdfb1413e8c0c6ff85e850475"
     }
    },
    "550f1e9f05764cf889bbc9446517012a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7771578357e14735a2bd11e1fb9b1c26",
      "placeholder": "​",
      "style": "IPY_MODEL_b0ccef862d214ab4b1ea9f4598f3d3f6",
      "value": "merges.txt: 100%"
     }
    },
    "587b2aab83e04afeb8c2be476272bf0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf090e65b4a54433af5dee4741729ac4",
      "max": 1252,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_36a55458338946218402aef65c685792",
      "value": 1252
     }
    },
    "5d828c6285a343af823bca17349f542b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5ff0fb30a51b4879a68af8d6aca44328": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62cf65ceb7814cd5a4f34cbd710b2d0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76642c05ab754cfa9af73e317a239576",
      "placeholder": "​",
      "style": "IPY_MODEL_501f55cee3eb450f934b2631e10acc6e",
      "value": " 8641/8641 [00:03&lt;00:00, 2784.48 examples/s]"
     }
    },
    "63063222c5854c888aea65436bef382a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "633278a4d63b4c2295c982fb59440e27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66a155e179674ed9abc0000363c8d38b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6765f6aca554497d869ada1fb40e79ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "697df8de2b2c48bc8bb45501a0de2657": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ae0872d96874841a3c672b862dbcf49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76642c05ab754cfa9af73e317a239576": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7771578357e14735a2bd11e1fb9b1c26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c8fa030782f4efeb5de5c000a579995": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88e95315ee9b4824b58ea88c83b8564f",
      "placeholder": "​",
      "style": "IPY_MODEL_ba275971fd824e54acb997c1769094ac",
      "value": " 1.25k/1.25k [00:00&lt;00:00, 31.1kB/s]"
     }
    },
    "8397490f483748058eeeba54798f848b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c8a50e422aa44d0815266d49d86fa81",
      "placeholder": "​",
      "style": "IPY_MODEL_de4f6435d4f44dc0928354060d8c6c54",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "88e95315ee9b4824b58ea88c83b8564f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b3e35915f8c45a7adf0cee037ff9338": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_550f1e9f05764cf889bbc9446517012a",
       "IPY_MODEL_c5ab38d71eed4a55914b30f6c4fd607f",
       "IPY_MODEL_efae6d14b8e447f0adfea7418f742f80"
      ],
      "layout": "IPY_MODEL_6ae0872d96874841a3c672b862dbcf49"
     }
    },
    "8c8a50e422aa44d0815266d49d86fa81": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c98495e51fa4368b8f117167a0750a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d82381fab1149efb895f1186d9d779d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08a8f6fd272e494ebfbfa7fa7599db63",
      "placeholder": "​",
      "style": "IPY_MODEL_0eb6522d093a4e669390fe3569a655bf",
      "value": "config.json: 100%"
     }
    },
    "a2c6f74126374f6fa39213ca51e413b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6fe5bf555da4834a50fa05425d2b034": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e879c6fd9e5b427eafb6f51c3173ec52",
      "placeholder": "​",
      "style": "IPY_MODEL_e6228b88e3ab46e08931a7d0a74f3a50",
      "value": " 1.73G/1.73G [00:16&lt;00:00, 202MB/s]"
     }
    },
    "a72c84dbe4054117bc264d2e28edb1ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ddc8e716fb8460fb1a6a66d0aea859c",
      "max": 1612610,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bfbf1e9ed7c84fe1ba1a29dda4ac034e",
      "value": 1612610
     }
    },
    "a9b8da9fd9784e46941c97817a46dc13": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae2fa80983824a708e0a71a6c1658b30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b0ccef862d214ab4b1ea9f4598f3d3f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba275971fd824e54acb997c1769094ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bb619c8c364f49ab9773a29756bb30ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb94101f510a4305aeacb3a6d3116614": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4712036c6a4145e1b555f7ac07f8aca0",
      "placeholder": "​",
      "style": "IPY_MODEL_ff4f9192b05b4ba8ae46eac8e7b694c2",
      "value": "Map: 100%"
     }
    },
    "bfbf1e9ed7c84fe1ba1a29dda4ac034e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c1770e9c8dcd4f07842192d3560135c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e3ebbc6db77a41c2a8fffc26ad5668b5",
       "IPY_MODEL_2543f71daf8e4e398b5335d5f7aa0679",
       "IPY_MODEL_62cf65ceb7814cd5a4f34cbd710b2d0c"
      ],
      "layout": "IPY_MODEL_bb619c8c364f49ab9773a29756bb30ec"
     }
    },
    "c5ab38d71eed4a55914b30f6c4fd607f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14115bbe026d4f01ba1732c632f3aac8",
      "max": 1270963,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f2393eaf86a542d7bc41a5281405a476",
      "value": 1270963
     }
    },
    "c660b2398d434605abfb512edc6eef28": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c861da9709624cc9982aee0f2f253048": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_363b09391b624b2d80b7cab8f4a3c647",
      "max": 1730074771,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6765f6aca554497d869ada1fb40e79ea",
      "value": 1730074771
     }
    },
    "cafd37d8e667444f94d8cd412d93174d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cf090e65b4a54433af5dee4741729ac4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d15fbafe8684472fafc5c4216cd02ea8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4e7476e357a046638227761b0dc854ce",
       "IPY_MODEL_27959938547e4241bcd1e307702b5843",
       "IPY_MODEL_09049bdf0320481f9a4454d0e6294534"
      ],
      "layout": "IPY_MODEL_9c98495e51fa4368b8f117167a0750a2"
     }
    },
    "d593924cbfc0475793b80c1e6982e3f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d72e5de082104630a60176f188267330": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d835b6dad89743e385ad3fd6aa2f5da6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c28cea2eac640c3b01fdef142741c77",
      "placeholder": "​",
      "style": "IPY_MODEL_da686f52ed15466fa0546b0a0f6d6ab1",
      "value": " 761/761 [00:00&lt;00:00, 11.6kB/s]"
     }
    },
    "da686f52ed15466fa0546b0a0f6d6ab1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "de4f6435d4f44dc0928354060d8c6c54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e21bb3a186484f21aa5a986986274f42": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e30fd5a1258b4437b2ae18fdac3bbff9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3ebbc6db77a41c2a8fffc26ad5668b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_243ca11ae232445e942dc676a088bab5",
      "placeholder": "​",
      "style": "IPY_MODEL_3a6bf958401d49448f334d18d5df740c",
      "value": "Map: 100%"
     }
    },
    "e4d8877bf36b474da6235c9414a24843": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e5a120e7233b487b97c2f1638aa382de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6228b88e3ab46e08931a7d0a74f3a50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e879c6fd9e5b427eafb6f51c3173ec52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efae6d14b8e447f0adfea7418f742f80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5a120e7233b487b97c2f1638aa382de",
      "placeholder": "​",
      "style": "IPY_MODEL_165c3185220f44b78429bf05254fdc42",
      "value": " 1.27M/1.27M [00:00&lt;00:00, 5.79MB/s]"
     }
    },
    "efb3ec3970974a1aa8377fc1bedea359": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9d82381fab1149efb895f1186d9d779d",
       "IPY_MODEL_40d32a8c13184baa9a4edb6b9b6139fa",
       "IPY_MODEL_d835b6dad89743e385ad3fd6aa2f5da6"
      ],
      "layout": "IPY_MODEL_5ff0fb30a51b4879a68af8d6aca44328"
     }
    },
    "f1ff745c43a54156a3f3f5816298a208": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e30fd5a1258b4437b2ae18fdac3bbff9",
      "max": 8641,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_22c294b3d2484e0b96ce730dcd5057b1",
      "value": 8641
     }
    },
    "f2393eaf86a542d7bc41a5281405a476": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f393fba7217e48b39444b1c7debe76ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b336e7298f049b187d1fbd6b92a4915",
      "placeholder": "​",
      "style": "IPY_MODEL_08e9f7e06576480996bd01741af87c9a",
      "value": " 1.61M/1.61M [00:00&lt;00:00, 7.49MB/s]"
     }
    },
    "f97846cf95304ebab1de87e3234486bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d72e5de082104630a60176f188267330",
      "placeholder": "​",
      "style": "IPY_MODEL_a2c6f74126374f6fa39213ca51e413b6",
      "value": " 8641/8641 [00:00&lt;00:00, 18473.36 examples/s]"
     }
    },
    "ff4f9192b05b4ba8ae46eac8e7b694c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
